import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import threading
from queue import Queue
import logging
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple, Any


class BaseStage(nn.Module):
	"""å¤„ç†é˜¶æ®µåŸºç±»ï¼Œæä¾›å…±äº«åŠŸèƒ½"""
	
	def __init__(self, name: str, device: str, node_comm=None):
		"""åˆå§‹åŒ–å¤„ç†é˜¶æ®µ"""
		super().__init__()
		self.name = name
		self.device = device
		self.node_comm = node_comm
		self.training = True
		
		# æ€§èƒ½ç›‘æ§
		self.compute_time = 0
		self.transfer_time = 0
		self.batch_count = 0
		self.last_error = None
		
		# è¾“å…¥è¾“å‡ºé˜Ÿåˆ—
		self.input_queue = Queue(maxsize=4)
		self.output_queue = Queue(maxsize=4)
		
		# å·¥ä½œçº¿ç¨‹
		self.worker_thread = None
		self.running = False
		
		# ğŸ”¥ æ–°å¢ï¼šæ—¥å¿—è®°å½•å™¨
		self.logger = logging.getLogger(f"{__name__}.{name}")
		
		# ğŸ”¥ æ–°å¢ï¼šé€šä¿¡ç»Ÿè®¡
		self.comm_success_count = 0
		self.comm_failure_count = 0
	
	def train(self, mode=True):
		"""è®¾ç½®è®­ç»ƒæ¨¡å¼"""
		self.training = mode
		return super().train(mode)
	
	def eval(self):
		"""è®¾ç½®è¯„ä¼°æ¨¡å¼"""
		self.training = False
		return super().eval()
	
	def reset_stats(self):
		"""é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
		self.compute_time = 0
		self.transfer_time = 0
		self.batch_count = 0
	
	def get_stats(self):
		"""è·å–æ€§èƒ½ç»Ÿè®¡"""
		avg_compute = self.compute_time / max(1, self.batch_count)
		avg_transfer = self.transfer_time / max(1, self.batch_count)
		return {
			'name': self.name,
			'device': self.device,
			'avg_compute_ms': avg_compute * 1000,
			'avg_transfer_ms': avg_transfer * 1000,
			'batch_count': self.batch_count,
			'last_error': str(self.last_error) if self.last_error else None
		}
	
	def start_worker(self):
		"""å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
		if self.worker_thread is not None and self.worker_thread.is_alive():
			return
		
		self.running = True
		self.worker_thread = threading.Thread(
			target=self._worker_loop,
			daemon=True,
			name=f"{self.name}_worker"
		)
		self.worker_thread.start()
	
	def stop_worker(self):
		"""åœæ­¢å·¥ä½œçº¿ç¨‹"""
		self.running = False
		if self.worker_thread and self.worker_thread.is_alive():
			self.worker_thread.join(timeout=3.0)
	
	def _worker_loop(self):
		"""å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯"""
		while self.running:
			try:
				# è·å–è¾“å…¥æ•°æ®
				if self.input_queue.empty():
					time.sleep(0.001)  # é¿å…CPUå¿™ç­‰
					continue
				
				inputs = self.input_queue.get(timeout=1.0)
				
				# å¤„ç†æ•°æ®
				outputs = self.process(*inputs)
				
				# å‘é€è¾“å‡º
				if outputs is not None:
					self.output_queue.put(outputs)
			
			except Exception as e:
				self.last_error = e
				
				time.sleep(1.0)  # é¿å…é”™è¯¯å¾ªç¯æ¶ˆè€—èµ„æº
	
	def process(self, *args, **kwargs):
		"""å¤„ç†é€»è¾‘(ç”±å­ç±»å®ç°)"""
		raise NotImplementedError("å­ç±»å¿…é¡»å®ç°processæ–¹æ³•")
	
	def forward(self, *args, **kwargs):
		"""åŒæ­¥å‰å‘å¤„ç†(ç”±å­ç±»å®ç°)"""
		raise NotImplementedError("å­ç±»å¿…é¡»å®ç°forwardæ–¹æ³•")
	
	def get_state_dict_prefix(self):
		"""è·å–å¸¦å‰ç¼€çš„å‚æ•°å­—å…¸"""
		return {}
	
	def log_communication_stats(self):
		"""è®°å½•é€šä¿¡çŠ¶æ€ç»Ÿè®¡"""
		if hasattr(self.node_comm, 'get_detailed_stats'):
			stats = self.node_comm.get_detailed_stats()
			
			self.logger.info(f"ğŸ“Š {self.name} é€šä¿¡çŠ¶æ€:")
			self.logger.info(f"  æˆåŠŸ: {self.comm_success_count}")
			self.logger.info(f"  å¤±è´¥: {self.comm_failure_count}")
			self.logger.info(f"  å‘é€å¤±è´¥: {stats.get('send_failures', 0)}")
			self.logger.info(f"  æ¥æ”¶å¤±è´¥: {stats.get('recv_failures', 0)}")
			
			if stats.get('send_failures', 0) + stats.get('recv_failures', 0) > 0:
				self.logger.warning("âš ï¸  æ£€æµ‹åˆ°é€šä¿¡é”™è¯¯ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥")
	
	def update_comm_stats(self, success: bool):
		"""æ›´æ–°é€šä¿¡ç»Ÿè®¡"""
		if success:
			self.comm_success_count += 1
		else:
			self.comm_failure_count += 1


class DummyStage(BaseStage):
	"""æç®€å ä½é˜¶æ®µ - åªå ç”¨GPUï¼Œä¸å‚ä¸ä¸»æµæ°´çº¿"""
	
	def __init__(self, model, device, node_comm=None, config=None):
		super().__init__("DummyStage", device, node_comm)
		
		# åˆ›å»ºä¸€ä¸ªæå°çš„"è£…é¥°"ç½‘ç»œï¼Œå ç”¨ä¸€ç‚¹GPUå†…å­˜
		self.dummy_layer = nn.Linear(10, 1).to(device)
	
	def process(self, dummy_input=None):
		"""æç®€å¤„ç† - ä»€ä¹ˆéƒ½ä¸åš"""
		# å¶å°”åšä¸ªæ— æ„ä¹‰çš„è®¡ç®—ï¼Œé˜²æ­¢è¢«ç³»ç»Ÿå›æ”¶
		if dummy_input is None:
			dummy_input = torch.randn(1, 10, device=self.device)
		
		_ = self.dummy_layer(dummy_input)  # æ‰”æ‰ç»“æœ
		return None
	
	def forward(self, *args, **kwargs):
		"""ä¸æ¥æ”¶ä»»ä½•æ•°æ®ï¼Œä¸å‘é€ä»»ä½•æ•°æ®"""
		time.sleep(0.001)  # è£…ä½œåœ¨å·¥ä½œ
		return None
	
	def get_state_dict_prefix(self):
		"""è¿”å›ç©ºçŠ¶æ€å­—å…¸"""
		return {}


# èŠ‚ç‚¹1 (GPU 0): æ•°æ®é¢„å¤„ç†+ROIæå–
class FrontendStage(BaseStage):
	"""å‰ç«¯å¤„ç†é˜¶æ®µ - å¤„ç†æ•°æ®é¢„å¤„ç†å’ŒROIæå–"""
	
	def __init__(self, model, device, node_comm=None, shared_components=None, config=None):
		super().__init__("FrontendStage", device, node_comm)
		self.config = config or {}
		
		# ä¼˜å…ˆä½¿ç”¨å…±äº«ç»„ä»¶
		self.preprocessor = None
		if shared_components and 'preprocessor' in shared_components:
			self.preprocessor = shared_components['preprocessor']
		
		elif hasattr(model, 'preprocessor') and model.preprocessor is not None:
			self.preprocessor = model.preprocessor
		
		else:
			# åªåœ¨å®Œå…¨æ‰¾ä¸åˆ°æ—¶æ‰åˆ›å»ºæ–°å®ä¾‹
			from data.processing import CTPreprocessor
			
			# ä»é…ç½®ä¸­è¯»å–é¢„å¤„ç†å‚æ•°
			preprocessing_config = self.config.get('preprocessing', {})
			roi_threshold = preprocessing_config.get('roi_threshold')
			roi_percentile = preprocessing_config.get('roi_percentile', 99.8)
			use_largest_cc = preprocessing_config.get('use_largest_cc', True)
			
			# åˆ›å»ºé¢„å¤„ç†å™¨å¹¶ä¼ é€’å‚æ•°
			self.preprocessor = CTPreprocessor(
				roi_threshold=roi_threshold,
				roi_percentile=roi_percentile,
				use_largest_cc=use_largest_cc,
				device=device,
			
			)
		
		# ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
		self.to(device)
	
	def process(self, batch):
		"""å¼‚æ­¥å¤„ç†æ‰¹æ¬¡æ•°æ®"""
		start_time = time.time()
		
		# ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
		images = batch['image'].to(self.device)
		labels = batch['label'].to(self.device) if 'label' in batch else None
		
		# é¢„å¤„ç†
		processed_images = []
		for i in range(images.shape[0]):
			# å½’ä¸€åŒ–
			norm_img = self.preprocessor.normalize(images[i].squeeze().cpu().numpy())
			# æå–è‚è„ROI
			liver_mask = self.preprocessor.extract_liver_roi(norm_img)
			
			# è½¬å›å¼ é‡
			norm_img_tensor = torch.from_numpy(norm_img).float().unsqueeze(0).to(self.device)
			liver_mask_tensor = torch.from_numpy(liver_mask).float().to(self.device)
			
			processed_images.append({
				'image': norm_img_tensor,
				'liver_mask': liver_mask_tensor,
				'case_id': batch.get('case_id', [f"case_{i}"])[i]
			})
		
		self.compute_time += time.time() - start_time
		self.batch_count += images.shape[0]
		
		# è¿”å›å¤„ç†åçš„æ•°æ®å’ŒåŸå§‹æ ‡ç­¾
		return processed_images, labels
	
	def forward(self, batch):
		"""åŒæ­¥å‰å‘å¤„ç†"""
		processed_images, labels = self.process(batch)
		
		# å°†ç»“æœå‘é€åˆ°ä¸‹ä¸€é˜¶æ®µ(GPU 1)
		if self.node_comm:
			next_rank = self.node_comm.rank + 1
			self.node_comm.send_tensor(
				torch.tensor([len(processed_images)], dtype=torch.long),
				dst_rank=next_rank
			)
			
			for i, proc_img in enumerate(processed_images):
				# å‘é€å›¾åƒ
				self.node_comm.send_tensor(proc_img['image'], dst_rank=next_rank)
				# å‘é€è‚è„æ©ç 
				self.node_comm.send_tensor(proc_img['liver_mask'], dst_rank=next_rank)
				# å‘é€case_id (ä½œä¸ºå…ƒæ•°æ®)
				meta = {'case_id': proc_img['case_id']}
				meta_tensor = torch.tensor([ord(c) for c in str(meta)],
				                           dtype=torch.uint8, device=self.device)
				self.node_comm.send_tensor(meta_tensor, dst_rank=next_rank)
			
			# å‘é€æ ‡ç­¾(å¦‚æœæœ‰)
			if labels is not None:
				has_labels = torch.tensor([1], dtype=torch.long, device=self.device)
				self.node_comm.send_tensor(has_labels, dst_rank=next_rank)
				self.node_comm.send_tensor(labels, dst_rank=next_rank)
			else:
				has_labels = torch.tensor([0], dtype=torch.long, device=self.device)
				self.node_comm.send_tensor(has_labels, dst_rank=next_rank)
		
		return processed_images, labels
	
	def get_state_dict_prefix(self):
		"""è·å–å¸¦å‰ç¼€çš„å‚æ•°å­—å…¸"""
		# åªä¿å­˜å¿…è¦çš„çŠ¶æ€
		state_dict = {}
		return state_dict


# èŠ‚ç‚¹1 (GPU 1): ä¸‰çº§é‡‡æ ·+Patchè°ƒåº¦
class PatchSchedulingStage(BaseStage):
	"""Patché‡‡æ ·å’Œè°ƒåº¦é˜¶æ®µ"""
	
	def __init__(self, model, device, node_comm=None, shared_components=None, config=None):
		super().__init__("PatchSchedulingStage", device, node_comm)
		self.config = config or {}
		
		# ä¼˜å…ˆä½¿ç”¨å…±äº«ç»„ä»¶
		self.tier_sampler = None
		if shared_components and 'tier_sampler' in shared_components:
			self.tier_sampler = shared_components['tier_sampler']
		
		elif hasattr(model, 'tier_sampler') and model.tier_sampler is not None:
			self.tier_sampler = model.tier_sampler
		
		else:
			# åªåœ¨å®Œå…¨æ‰¾ä¸åˆ°æ—¶æ‰åˆ›å»ºæ–°å®ä¾‹
			from data.tier_sampling import TierSampler
			
			# ä»é…ç½®ä¸­è¯»å–é‡‡æ ·å‚æ•°
			sampling_config = self.config.get('smart_sampling', {})
			
			# åˆ›å»ºé‡‡æ ·å™¨å¹¶ä¼ é€’å‚æ•°
			self.tier_sampler = TierSampler(
				tier0_size=sampling_config.get('tier0_size', 256),
				tier1_size=sampling_config.get('tier1_size', 96),
				tier2_size=sampling_config.get('tier2_size', 64),
				max_tier1=sampling_config.get('maxtier1', 10),
				max_tier2=sampling_config.get('maxtier2', 20)
			)
		
		# ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
		self.to(device)
		
		# ç¼“å­˜å·²å¤„ç†çš„patches
		self.patches_cache = {}
	
	def process(self, processed_images, labels):
		"""å¤„ç†é¢„å¤„ç†åçš„å›¾åƒæ•°æ®"""
		start_time = time.time()
		
		all_patches = []
		case_patches = {}
		
		# å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œä¸‰çº§é‡‡æ ·
		for i, proc_img in enumerate(processed_images):
			# è·å–case_id
			case_id = proc_img['case_id']
			
			# æ£€æŸ¥ç¼“å­˜
			if case_id in self.patches_cache:
				patches = self.patches_cache[case_id]
			else:
				# æå–åˆ°CPUè¿›è¡Œé‡‡æ ·
				image_np = proc_img['image'].squeeze().cpu().numpy()
				liver_mask_np = proc_img['liver_mask'].cpu().numpy()
				
				# è·å–å½“å‰æ ·æœ¬çš„æ ‡ç­¾
				label_np = None
				if labels is not None:
					label_np = labels[i].cpu().numpy() if labels.shape[0] > i else None
				
				# åº”ç”¨ä¸‰çº§é‡‡æ ·
				patches = self.tier_sampler.sample(
					image_np, label_np, liver_mask_np, case_id=case_id
				)
				
				# æ›´æ–°ç¼“å­˜
				self.patches_cache[case_id] = patches
			
			all_patches.extend(patches)
			case_patches[case_id] = patches
		
		# æŒ‰Tierå¯¹patchesè¿›è¡Œæ’åº
		sorted_patches = sorted(all_patches, key=lambda p: p['tier'])
		
		self.compute_time += time.time() - start_time
		self.batch_count += len(processed_images)
		
		return sorted_patches, case_patches
	
	def forward(self, processed_images=None, labels=None):
		"""åŒæ­¥å‰å‘å¤„ç†"""
		if processed_images is None and self.node_comm:
			# ä»ä¸Šä¸€é˜¶æ®µ(GPU 0)æ¥æ”¶æ•°æ®
			prev_rank = self.node_comm.rank - 1
			
			# æ¥æ”¶æ ·æœ¬æ•°é‡
			count_tensor = self.node_comm.recv_tensor(
				src_rank=prev_rank,
				dtype=torch.long,
				device=self.device
			)
			count = count_tensor.item()
			
			# æ¥æ”¶æ¯ä¸ªæ ·æœ¬
			processed_images = []
			for i in range(count):
				# æ¥æ”¶å›¾åƒ
				image = self.node_comm.recv_tensor(
					src_rank=prev_rank,
					dtype=torch.float32,
					device=self.device
				)
				
				# æ¥æ”¶è‚è„æ©ç 
				liver_mask = self.node_comm.recv_tensor(
					src_rank=prev_rank,
					dtype=torch.float32,
					device=self.device
				)
				
				# æ¥æ”¶å…ƒæ•°æ®
				meta_tensor = self.node_comm.recv_tensor(
					src_rank=prev_rank,
					dtype=torch.uint8,
					device=self.device
				)
				meta_str = ''.join([chr(c) for c in meta_tensor.cpu().numpy()])
				meta = eval(meta_str)  # å®‰å…¨é—®é¢˜ï¼šå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨æ›´å®‰å…¨çš„åºåˆ—åŒ–æ–¹æ³•
				
				processed_images.append({
					'image': image,
					'liver_mask': liver_mask,
					'case_id': meta['case_id']
				})
			
			# æ¥æ”¶æ ‡ç­¾
			has_labels = self.node_comm.recv_tensor(
				src_rank=prev_rank,
				dtype=torch.long,
				device=self.device
			).item()
			
			if has_labels:
				labels = self.node_comm.recv_tensor(
					src_rank=prev_rank,
					device=self.device
				)
			else:
				labels = None
		
		# å¤„ç†æ•°æ®
		patches, case_patches = self.process(processed_images, labels)
		
		# å°†patcheså‘é€åˆ°CHåˆ†æ”¯(GPU 2)å’Œç©ºé—´åˆ†æ”¯(GPU 3) - ä½¿ç”¨æ–°çš„é€šä¿¡æ–¹å¼
		if self.node_comm:
			# å‡†å¤‡å‘é€æ•°æ®ï¼šè½¬æ¢ä¸ºList[Tuple[torch.Tensor, int]]æ ¼å¼
			patch_data = []
			for patch in patches:
				# è½¬æ¢patchä¸ºå¼ é‡
				if isinstance(patch['image'], np.ndarray):
					patch_tensor = torch.from_numpy(patch['image']).float().unsqueeze(0).to(self.device)
				else:
					patch_tensor = patch['image'].to(self.device)
				
				tier = int(patch['tier'])
				patch_data.append((patch_tensor, tier))
			
			# å‘é€åˆ°CHåˆ†æ”¯ (GPU 2)
			ch_rank = self.node_comm.rank + 1
			success_ch = self._send_patches_to_branch(patch_data, ch_rank, "CH", tag=50)
			
			# å‘é€åˆ°ç©ºé—´åˆ†æ”¯ (GPU 3)
			spatial_rank = ch_rank + 1
			success_spatial = self._send_patches_to_branch(patch_data, spatial_rank, "Spatial", tag=60)
			
			# è®°å½•å‘é€çŠ¶æ€
			if success_ch and success_spatial:
				self.logger.debug(f"âœ… Patcheså‘é€æˆåŠŸ: {len(patches)}ä¸ªpatchesåˆ°ä¸¤ä¸ªåˆ†æ”¯")
				self.update_comm_stats(True)
			else:
				self.logger.warning(f"âš ï¸  Patcheså‘é€éƒ¨åˆ†å¤±è´¥: CH={success_ch}, Spatial={success_spatial}")
				self.update_comm_stats(False)
		
		return patches, case_patches
	
	def _send_patches_to_branch(self, patch_data: List[Tuple[torch.Tensor, int]],
	                            dst_rank: int, branch_name: str, tag: int = 0) -> bool:
		"""å‘é€patchesåˆ°æŒ‡å®šåˆ†æ”¯"""
		try:
			# ğŸ”¥ ä¼˜å…ˆä½¿ç”¨æ–°çš„å¤æ‚æ•°æ®ç±»å‹ä¼ è¾“
			if hasattr(self.node_comm, 'send_tensor_tuple_list'):
				success = self.node_comm.send_tensor_tuple_list(patch_data, dst_rank=dst_rank, tag=tag)
				if success:
					self.logger.debug(f"âœ… {branch_name}åˆ†æ”¯å‘é€æˆåŠŸ: {len(patch_data)}ä¸ªpatches")
					return True
				else:
					self.logger.warning(f"âš ï¸  {branch_name}åˆ†æ”¯å‘é€å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
			
			elif hasattr(self.node_comm, 'send_data'):
				success = self.node_comm.send_data(patch_data, dst_rank=dst_rank, tag=tag, reliable=True)
				if success:
					self.logger.debug(f"âœ… {branch_name}åˆ†æ”¯å‘é€æˆåŠŸ(é€šç”¨æ¨¡å¼): {len(patch_data)}ä¸ªpatches")
					return True
				else:
					self.logger.warning(f"âš ï¸  {branch_name}åˆ†æ”¯å‘é€å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
			
			# å›é€€åˆ°åŸæœ‰å‘é€æ–¹å¼
			return self._fallback_send_patches(patch_data, dst_rank, branch_name)
		
		except Exception as e:
			self.logger.error(f"âŒ {branch_name}åˆ†æ”¯å‘é€å¼‚å¸¸: {e}")
			return self._fallback_send_patches(patch_data, dst_rank, branch_name)
	
	def _fallback_send_patches(self, patch_data: List[Tuple[torch.Tensor, int]],
	                           dst_rank: int, branch_name: str) -> bool:
		"""å›é€€å‘é€æ–¹å¼"""
		try:
			# å‘é€patchesæ•°é‡
			count_tensor = torch.tensor([len(patch_data)], dtype=torch.long, device=self.device)
			self.node_comm.send_tensor(count_tensor, dst_rank=dst_rank, reliable=False)
			
			# é€ä¸ªå‘é€patches
			for patch_tensor, tier in patch_data:
				self.node_comm.send_tensor(patch_tensor, dst_rank=dst_rank, reliable=False)
				tier_tensor = torch.tensor([tier], dtype=torch.long, device=self.device)
				self.node_comm.send_tensor(tier_tensor, dst_rank=dst_rank, reliable=False)
			
			self.logger.debug(f"âœ… {branch_name}åˆ†æ”¯å‘é€æˆåŠŸ(å›é€€æ¨¡å¼): {len(patch_data)}ä¸ªpatches")
			return True
		
		except Exception as e:
			self.logger.error(f"âŒ {branch_name}åˆ†æ”¯å›é€€å‘é€ä¹Ÿå¤±è´¥: {e}")
			return False
	
	def get_state_dict_prefix(self):
		"""è·å–å¸¦å‰ç¼€çš„å‚æ•°å­—å…¸"""
		# åªä¿å­˜å¿…è¦çš„çŠ¶æ€
		state_dict = {}
		return state_dict


# èŠ‚ç‚¹1 (GPU 2): CHåˆ†æ”¯å®Œæ•´å¤„ç†
class CHProcessingStage(BaseStage):
	"""CHå¤„ç†é˜¶æ®µ - å¤„ç†CHåˆ†æ”¯"""
	
	def __init__(self, model, device, node_comm=None, shared_components=None, config=None):
		super().__init__("CHProcessingStage", device, node_comm)
		self.config = config or {}
		
		# æå–CHåˆ†æ”¯ç»„ä»¶ - è¿™äº›é€šå¸¸æ¥è‡ªæ¨¡å‹ï¼Œä¸æ˜¯ç‹¬ç«‹åˆå§‹åŒ–çš„
		self.ch_branch = model.ch_branch
		self.tier_params = model.tier_params
		
		# ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
		self.ch_branch.to(device)
	
	def process(self, patches, tiers):
		"""å¤„ç†patches - å®Œæ•´å®ç°CHåˆ†æ”¯"""
		start_time = time.time()
		
		ch_features = []
		processed_tiers = []
		
		for patch, tier in zip(patches, tiers):
			try:
				# 1. è®¾ç½®å½“å‰tierçš„CHå‚æ•°
				if hasattr(self.ch_branch, 'set_tier'):
					self.ch_branch.set_tier(tier)
				
				# 2. è·å–tierç‰¹å®šçš„å‚æ•°
				tier_params = self.tier_params.get(tier, {})
				r_scale = tier_params.get('r_scale', 1.0)
				
				# 3. ç¡®ä¿è¾“å…¥æ˜¯å¼ é‡æ ¼å¼
				if isinstance(patch, np.ndarray):
					patch_tensor = torch.from_numpy(patch).float().unsqueeze(0).to(self.device)
				else:
					patch_tensor = patch.to(self.device)
				
				# 4. CHåˆ†æ”¯å‰å‘ä¼ æ’­
				if hasattr(self.ch_branch, '__call__'):
					# æ£€æŸ¥ch_branchæ˜¯å¦æ”¯æŒr_scaleå‚æ•°
					import inspect
					sig = inspect.signature(self.ch_branch.forward)
					if 'r_scale' in sig.parameters:
						ch_output = self.ch_branch(patch_tensor, r_scale=r_scale)
					else:
						ch_output = self.ch_branch(patch_tensor)
				else:
					ch_output = patch_tensor  # å¤‡ç”¨æ–¹æ¡ˆ
				
				ch_features.append(ch_output)
				processed_tiers.append(tier)
			
			except Exception as e:
				print(f"CH processing failed for tier {tier}: {e}")
				continue
		
		self.compute_time += time.time() - start_time
		self.batch_count += len(patches)
		
		return ch_features, processed_tiers
	
	def forward(self, patches=None, tiers=None):
		"""åŒæ­¥å‰å‘å¤„ç† - ä¼˜åŒ–ç‰ˆ"""
		if patches is None and self.node_comm:
			# ä»PatchSchedulingStageæ¥æ”¶æ•°æ®
			prev_rank = self.node_comm.rank - 1
			
			# 1. æ¥æ”¶æ•°æ®åŒ…æ•°é‡
			count_tensor = self.node_comm.recv_tensor(
				src_rank=prev_rank,
				dtype=torch.long,
				device=self.device
			)
			count = count_tensor.item()
			
			# 2. æ‰¹é‡æ¥æ”¶æ‰€æœ‰patches
			patches = []
			tiers = []
			
			for i in range(count):
				# æ¥æ”¶patchå¼ é‡
				patch_tensor = self.node_comm.recv_tensor(
					src_rank=prev_rank,
					dtype=torch.float32,
					device=self.device
				)
				
				# æ¥æ”¶tierä¿¡æ¯
				tier_tensor = self.node_comm.recv_tensor(
					src_rank=prev_rank,
					dtype=torch.long,
					device=self.device
				)
				
				patches.append(patch_tensor)
				tiers.append(tier_tensor.item())
		
		# å¤„ç†æ•°æ®
		ch_features, processed_tiers = self.process(patches, tiers)
		
		# å‘é€åˆ°FeatureFusionStage - ä½¿ç”¨æ–°çš„å¤æ‚æ•°æ®ç±»å‹ä¼ è¾“
		if self.node_comm:
			# ç›®æ ‡rankè®¡ç®—ï¼ˆèŠ‚ç‚¹2çš„GPU 4ï¼‰
			fusion_rank = self.node_comm.node_ranks[1] if hasattr(self.node_comm,
			                                                      'node_ranks') else self.node_comm.rank + 2
			
			try:
				# ğŸ”¥ æ–°åŠŸèƒ½ï¼šç›´æ¥å‘é€List[Tuple[torch.Tensor, int]]æ ¼å¼
				ch_data = [(ch_feat, tier) for ch_feat, tier in zip(ch_features, processed_tiers)]
				
				# æ£€æŸ¥æ˜¯å¦æ”¯æŒæ–°çš„å¤æ‚æ•°æ®ç±»å‹ä¼ è¾“
				if hasattr(self.node_comm, 'send_tensor_tuple_list'):
					# ä½¿ç”¨ä¸“é—¨çš„æ–¹æ³•å‘é€tensor-tupleåˆ—è¡¨
					success = self.node_comm.send_tensor_tuple_list(ch_data, dst_rank=fusion_rank, tag=100)
					
					if success:
						self.logger.debug(f"âœ… CHç‰¹å¾å‘é€æˆåŠŸ: {len(ch_data)}ä¸ªç‰¹å¾")
						self.update_comm_stats(True)
					else:
						self.logger.error("âŒ CHç‰¹å¾å‘é€å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
						self.update_comm_stats(False)
						# å›é€€åˆ°åŸæœ‰å‘é€æ–¹å¼
						self._fallback_send_features(ch_features, processed_tiers, fusion_rank)
				
				elif hasattr(self.node_comm, 'send_data'):
					# ä½¿ç”¨é€šç”¨çš„å¤æ‚æ•°æ®å‘é€æ–¹æ³•
					success = self.node_comm.send_data(ch_data, dst_rank=fusion_rank, tag=100, reliable=True)
					
					if success:
						self.logger.debug(f"âœ… CHç‰¹å¾å‘é€æˆåŠŸ(é€šç”¨æ¨¡å¼): {len(ch_data)}ä¸ªç‰¹å¾")
						self.update_comm_stats(True)
					else:
						self.logger.error("âŒ CHç‰¹å¾å‘é€å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
						self.update_comm_stats(False)
						self._fallback_send_features(ch_features, processed_tiers, fusion_rank)
				
				else:
					# å›é€€åˆ°åŸæœ‰å‘é€æ–¹å¼
					self.logger.warning("âš ï¸  ä½¿ç”¨åŸæœ‰å‘é€æ–¹å¼(ä¸æ”¯æŒå¤æ‚æ•°æ®ç±»å‹)")
					self._fallback_send_features(ch_features, processed_tiers, fusion_rank)
			
			except Exception as e:
				self.logger.error(f"âŒ CHç‰¹å¾å‘é€å¼‚å¸¸: {e}")
				self.update_comm_stats(False)
				# å›é€€åˆ°åŸæœ‰å‘é€æ–¹å¼
				self._fallback_send_features(ch_features, processed_tiers, fusion_rank)
		
		return ch_features, processed_tiers
	
	def _fallback_send_features(self, ch_features, processed_tiers, fusion_rank):
		"""å›é€€å‘é€æ–¹å¼ - å…¼å®¹åŸæœ‰é€šä¿¡æ–¹å¼"""
		try:
			# å‘é€ç‰¹å¾æ•°é‡
			count_tensor = torch.tensor([len(ch_features)], dtype=torch.long, device=self.device)
			self.node_comm.send_tensor(count_tensor, dst_rank=fusion_rank, reliable=False)
			
			# é€ä¸ªå‘é€ç‰¹å¾
			for ch_feat, tier in zip(ch_features, processed_tiers):
				self.node_comm.send_tensor(ch_feat, dst_rank=fusion_rank, reliable=False)
				
				tier_tensor = torch.tensor([tier], dtype=torch.long, device=self.device)
				self.node_comm.send_tensor(tier_tensor, dst_rank=fusion_rank, reliable=False)
			
			self.logger.debug(f"âœ… CHç‰¹å¾å‘é€æˆåŠŸ(å›é€€æ¨¡å¼): {len(ch_features)}ä¸ªç‰¹å¾")
		
		except Exception as e:
			self.logger.error(f"âŒ å›é€€å‘é€ä¹Ÿå¤±è´¥: {e}")
	
	def get_state_dict_prefix(self):
		"""è·å–å¸¦å‰ç¼€çš„å‚æ•°å­—å…¸"""
		# ä¿å­˜CHåˆ†æ”¯å‚æ•°
		state_dict = {}
		for name, param in self.ch_branch.state_dict().items():
			state_dict[f'ch_branch.{name}'] = param
		return state_dict


# èŠ‚ç‚¹1 (GPU 3): ç©ºé—´åˆ†æ”¯å®Œæ•´å¤„ç†
class SpatialFusionStage(BaseStage):
	"""ç©ºé—´èåˆé˜¶æ®µ - å¤„ç†ç©ºé—´åˆ†æ”¯"""
	
	def __init__(self, model, device, node_comm=None, shared_components=None, config=None):
		super().__init__("SpatialFusionStage", device, node_comm)
		self.config = config or {}
		
		# æå–ç©ºé—´åˆ†æ”¯ç»„ä»¶ - è¿™äº›é€šå¸¸æ¥è‡ªæ¨¡å‹ï¼Œä¸æ˜¯ç‹¬ç«‹åˆå§‹åŒ–çš„
		self.spatial_branch = model.spatial_branch
		self.edge_enhance = model.edge_enhance
		
		# ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
		self.spatial_branch.to(device)
		self.edge_enhance.to(device)
		
		# æ·»åŠ é€šé“é€‚é…å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
		self.channel_adapter = None
	
	def _build_channel_adapter(self, input_channels, target_channels, device):
		"""æ„å»ºé€šé“é€‚é…å™¨"""
		if input_channels != target_channels:
			self.channel_adapter = nn.Conv3d(
				input_channels, target_channels,
				kernel_size=1, bias=False
			).to(device)
	
	def process(self, patches, tiers):
		"""å¤„ç†patches - å®Œæ•´å®ç°ç©ºé—´åˆ†æ”¯"""
		start_time = time.time()
		
		spatial_features = []
		processed_tiers = []
		
		for patch, tier in zip(patches, tiers):
			try:
				# ç¡®ä¿è¾“å…¥æ˜¯å¼ é‡æ ¼å¼
				if isinstance(patch, np.ndarray):
					patch_tensor = torch.from_numpy(patch).float().unsqueeze(0).to(self.device)
				else:
					patch_tensor = patch.to(self.device)
				
				# 1. è¾¹ç¼˜å¢å¼ºå¤„ç†
				edge_features = self.edge_enhance(patch_tensor)
				
				# 2. ç©ºé—´ç‰¹å¾æå–
				spatial_feat = self.spatial_branch(patch_tensor)
				
				# 3. ç‰¹å¾èåˆï¼ˆå¦‚æœéœ€è¦ï¼‰
				if edge_features.shape[1] != spatial_feat.shape[1]:
					# åŠ¨æ€æ„å»ºé€šé“é€‚é…å™¨
					if self.channel_adapter is None:
						self._build_channel_adapter(
							edge_features.shape[1],
							spatial_feat.shape[1],
							self.device
						)
					
					if self.channel_adapter is not None:
						edge_features = self.channel_adapter(edge_features)
					else:
						# ç®€å•çš„é€šé“è°ƒæ•´
						edge_features = F.adaptive_avg_pool3d(edge_features, (1, 1, 1))
						edge_features = F.interpolate(edge_features, size=spatial_feat.shape[2:])
						edge_features = edge_features.expand_as(spatial_feat)
				
				# ç»„åˆç©ºé—´ç‰¹å¾å’Œè¾¹ç¼˜ç‰¹å¾
				combined_features = spatial_feat + edge_features
				
				spatial_features.append(combined_features)
				processed_tiers.append(tier)
			
			except Exception as e:
				print(f"Spatial processing failed for tier {tier}: {e}")
				continue
		
		self.compute_time += time.time() - start_time
		self.batch_count += len(patches)
		
		return spatial_features, processed_tiers
	
	def forward(self, patches=None, tiers=None):
		"""åŒæ­¥å‰å‘å¤„ç†"""
		if patches is None and self.node_comm:
			# ä»PatchSchedulingStage(GPU 1)æ¥æ”¶æ•°æ® - ä½¿ç”¨æ–°çš„é€šä¿¡æ–¹å¼
			prev_rank = self.node_comm.rank - 2  # PatchSchedulingStageåœ¨GPU 1
			
			patches = []
			tiers = []
			
			try:
				# ğŸ”¥ æ–°åŠŸèƒ½ï¼šæ¥æ”¶List[Tuple[torch.Tensor, int]]æ ¼å¼
				if hasattr(self.node_comm, 'recv_tensor_tuple_list'):
					patch_data = self.node_comm.recv_tensor_tuple_list(
						src_rank=prev_rank,
						tag=60  # å¯¹åº”å‘é€æ—¶çš„tag
					)
					
					if patch_data is not None:
						patches = [item[0] for item in patch_data]
						tiers = [item[1] for item in patch_data]
						self.logger.debug(f"âœ… ç©ºé—´åˆ†æ”¯æ¥æ”¶æˆåŠŸ: {len(patch_data)}ä¸ªpatches")
						self.update_comm_stats(True)
					else:
						self.logger.error("âŒ ç©ºé—´åˆ†æ”¯æ¥æ”¶å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
						self.update_comm_stats(False)
						patches, tiers = self._fallback_recv_patches(prev_rank)
				
				elif hasattr(self.node_comm, 'recv_data'):
					patch_data = self.node_comm.recv_data(
						src_rank=prev_rank,
						tag=60,
						reliable=True
					)
					
					if patch_data is not None and isinstance(patch_data, list):
						patches = [item[0] for item in patch_data]
						tiers = [item[1] for item in patch_data]
						self.logger.debug(f"âœ… ç©ºé—´åˆ†æ”¯æ¥æ”¶æˆåŠŸ(é€šç”¨æ¨¡å¼): {len(patch_data)}ä¸ªpatches")
						self.update_comm_stats(True)
					else:
						self.logger.error("âŒ ç©ºé—´åˆ†æ”¯æ¥æ”¶å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
						self.update_comm_stats(False)
						patches, tiers = self._fallback_recv_patches(prev_rank)
				
				else:
					# å›é€€åˆ°åŸæœ‰æ¥æ”¶æ–¹å¼
					self.logger.warning("âš ï¸  ä½¿ç”¨åŸæœ‰æ¥æ”¶æ–¹å¼(ä¸æ”¯æŒå¤æ‚æ•°æ®ç±»å‹)")
					patches, tiers = self._fallback_recv_patches(prev_rank)
			
			except Exception as e:
				self.logger.error(f"âŒ ç©ºé—´åˆ†æ”¯æ¥æ”¶å¼‚å¸¸: {e}")
				self.update_comm_stats(False)
				patches, tiers = self._fallback_recv_patches(prev_rank)
		
		# å¤„ç†patches
		spatial_features, processed_tiers = self.process(patches, tiers)
		
		# å°†ç©ºé—´ç‰¹å¾å‘é€åˆ°ç‰¹å¾èåˆé˜¶æ®µ(GPU 4, èŠ‚ç‚¹2) - ä½¿ç”¨æ–°çš„é€šä¿¡æ–¹å¼
		if self.node_comm:
			# ç‰¹å¾èåˆé˜¶æ®µåœ¨èŠ‚ç‚¹2
			fusion_rank = self.node_comm.node_ranks[1] if hasattr(self.node_comm,
			                                                      'node_ranks') else self.node_comm.rank + 1
			
			try:
				# ğŸ”¥ æ–°åŠŸèƒ½ï¼šå‘é€List[Tuple[torch.Tensor, int]]æ ¼å¼
				spatial_data = [(feature, tier) for feature, tier in zip(spatial_features, processed_tiers)]
				
				if hasattr(self.node_comm, 'send_tensor_tuple_list'):
					success = self.node_comm.send_tensor_tuple_list(spatial_data, dst_rank=fusion_rank, tag=110)
					
					if success:
						self.logger.debug(f"âœ… ç©ºé—´ç‰¹å¾å‘é€æˆåŠŸ: {len(spatial_data)}ä¸ªç‰¹å¾")
						self.update_comm_stats(True)
					else:
						self.logger.error("âŒ ç©ºé—´ç‰¹å¾å‘é€å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
						self.update_comm_stats(False)
						self._fallback_send_spatial_features(spatial_features, processed_tiers, fusion_rank)
				
				elif hasattr(self.node_comm, 'send_data'):
					success = self.node_comm.send_data(spatial_data, dst_rank=fusion_rank, tag=110, reliable=True)
					
					if success:
						self.logger.debug(f"âœ… ç©ºé—´ç‰¹å¾å‘é€æˆåŠŸ(é€šç”¨æ¨¡å¼): {len(spatial_data)}ä¸ªç‰¹å¾")
						self.update_comm_stats(True)
					else:
						self.logger.error("âŒ ç©ºé—´ç‰¹å¾å‘é€å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
						self.update_comm_stats(False)
						self._fallback_send_spatial_features(spatial_features, processed_tiers, fusion_rank)
				
				else:
					# å›é€€åˆ°åŸæœ‰å‘é€æ–¹å¼
					self.logger.warning("âš ï¸  ä½¿ç”¨åŸæœ‰å‘é€æ–¹å¼(ä¸æ”¯æŒå¤æ‚æ•°æ®ç±»å‹)")
					self._fallback_send_spatial_features(spatial_features, processed_tiers, fusion_rank)
			
			except Exception as e:
				self.logger.error(f"âŒ ç©ºé—´ç‰¹å¾å‘é€å¼‚å¸¸: {e}")
				self.update_comm_stats(False)
				self._fallback_send_spatial_features(spatial_features, processed_tiers, fusion_rank)
		
		return spatial_features, processed_tiers
	
	def _fallback_recv_patches(self, prev_rank):
		"""å›é€€æ¥æ”¶æ–¹å¼"""
		patches = []
		tiers = []
		
		try:
			# æ¥æ”¶patchesæ•°é‡
			count_tensor = self.node_comm.recv_tensor(
				src_rank=prev_rank,
				dtype=torch.long,
				device=self.device,
				reliable=False
			)
			
			if count_tensor is None:
				self.logger.error("æ— æ³•æ¥æ”¶patchesæ•°é‡")
				return patches, tiers
			
			count = count_tensor.item()
			
			# æ¥æ”¶æ¯ä¸ªpatch
			for i in range(count):
				# æ¥æ”¶patchå›¾åƒ
				patch_tensor = self.node_comm.recv_tensor(
					src_rank=prev_rank,
					dtype=torch.float32,
					device=self.device,
					reliable=False
				)
				
				# æ¥æ”¶tierä¿¡æ¯
				tier_tensor = self.node_comm.recv_tensor(
					src_rank=prev_rank,
					dtype=torch.long,
					device=self.device,
					reliable=False
				)
				
				if patch_tensor is not None and tier_tensor is not None:
					patches.append(patch_tensor)
					tiers.append(tier_tensor.item())
				else:
					self.logger.warning(f"patchæˆ–tieræ¥æ”¶å¤±è´¥: ç¬¬{i}ä¸ª")
			
			self.logger.debug(f"âœ… ç©ºé—´åˆ†æ”¯æ¥æ”¶æˆåŠŸ(å›é€€æ¨¡å¼): {len(patches)}ä¸ªpatches")
		
		except Exception as e:
			self.logger.error(f"âŒ å›é€€æ¥æ”¶ä¹Ÿå¤±è´¥: {e}")
		
		return patches, tiers
	
	def _fallback_send_spatial_features(self, spatial_features, processed_tiers, fusion_rank):
		"""å›é€€å‘é€æ–¹å¼"""
		try:
			# å‘é€featuresæ•°é‡
			count_tensor = torch.tensor([len(spatial_features)], dtype=torch.long, device=self.device)
			self.node_comm.send_tensor(count_tensor, dst_rank=fusion_rank, reliable=False)
			
			# é€ä¸ªå‘é€ç‰¹å¾
			for feature, tier in zip(spatial_features, processed_tiers):
				self.node_comm.send_tensor(feature, dst_rank=fusion_rank, reliable=False)
				tier_tensor = torch.tensor([tier], dtype=torch.long, device=self.device)
				self.node_comm.send_tensor(tier_tensor, dst_rank=fusion_rank, reliable=False)
			
			self.logger.debug(f"âœ… ç©ºé—´ç‰¹å¾å‘é€æˆåŠŸ(å›é€€æ¨¡å¼): {len(spatial_features)}ä¸ªç‰¹å¾")
		
		except Exception as e:
			self.logger.error(f"âŒ å›é€€å‘é€ä¹Ÿå¤±è´¥: {e}")
	
	def get_state_dict_prefix(self):
		"""è·å–å¸¦å‰ç¼€çš„å‚æ•°å­—å…¸"""
		# ä¿å­˜ç©ºé—´åˆ†æ”¯å‚æ•°
		state_dict = {}
		for name, param in self.spatial_branch.state_dict().items():
			state_dict[f'spatial_branch.{name}'] = param
		for name, param in self.edge_enhance.state_dict().items():
			state_dict[f'edge_enhance.{name}'] = param
		if self.channel_adapter is not None:
			for name, param in self.channel_adapter.state_dict().items():
				state_dict[f'channel_adapter.{name}'] = param
		return state_dict


# èŠ‚ç‚¹2 (GPU 4): ç‰¹å¾èåˆ
class FeatureFusionStage(BaseStage):
	"""ç‰¹å¾èåˆé˜¶æ®µ"""
	
	def __init__(self, model, device, node_comm=None, shared_components=None, config=None):
		super().__init__("FeatureFusionStage", device, node_comm)
		self.config = config or {}
		
		# æå–ç‰¹å¾èåˆç»„ä»¶ - è¿™äº›é€šå¸¸æ¥è‡ªæ¨¡å‹ï¼Œä¸æ˜¯ç‹¬ç«‹åˆå§‹åŒ–çš„
		self.attention_fusion = model.attention_fusion
		
		# ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
		self.attention_fusion.to(device)
		
		# ç‰¹å¾ç¼“å­˜
		self.ch_features_cache = {}
		self.spatial_features_cache = {}
		self.tiers_cache = {}
		self.fused_features = {}
	
	def process(self, ch_features, spatial_features, tiers):
		"""èåˆCHç‰¹å¾å’Œç©ºé—´ç‰¹å¾ - å®Œæ•´å®ç°"""
		start_time = time.time()
		
		fused_features = []
		
		# ç¡®ä¿ç‰¹å¾æ•°é‡åŒ¹é…
		min_len = min(len(ch_features), len(spatial_features))
		
		for i in range(min_len):
			ch_feat = ch_features[i]
			spatial_feat = spatial_features[i]
			tier = tiers[i] if i < len(tiers) else 0
			
			try:
				# ç‰¹å¾ç»´åº¦å¯¹é½
				if ch_feat.shape != spatial_feat.shape:
					# è°ƒæ•´ç©ºé—´ç»´åº¦
					if ch_feat.shape[2:] != spatial_feat.shape[2:]:
						spatial_feat = F.interpolate(
							spatial_feat,
							size=ch_feat.shape[2:],
							mode='trilinear',
							align_corners=False
						)
					
					# è°ƒæ•´é€šé“ç»´åº¦
					if ch_feat.shape[1] != spatial_feat.shape[1]:
						if ch_feat.shape[1] > spatial_feat.shape[1]:
							# æ‰©å±•spatialç‰¹å¾é€šé“
							pad_channels = ch_feat.shape[1] - spatial_feat.shape[1]
							spatial_feat = F.pad(spatial_feat, (0, 0, 0, 0, 0, 0, 0, pad_channels))
						else:
							# è£å‰ªspatialç‰¹å¾é€šé“
							spatial_feat = spatial_feat[:, :ch_feat.shape[1]]
				
				# åº”ç”¨æ³¨æ„åŠ›èåˆ
				fused = self.attention_fusion(ch_feat, spatial_feat)
				
				fused_features.append((fused, tier))
			
			except Exception as e:
				print(f"Feature fusion failed for tier {tier}: {e}")
				continue
		
		self.compute_time += time.time() - start_time
		self.batch_count += len(ch_features)
		
		return fused_features
	
	def forward(self, ch_features=None, spatial_features=None, tiers=None):
		"""åŒæ­¥å‰å‘å¤„ç†"""
		if ch_features is None and self.node_comm:
			# æ¥æ”¶CHç‰¹å¾(ä»èŠ‚ç‚¹1çš„GPU 2) - ä½¿ç”¨æ–°çš„å¤æ‚æ•°æ®ç±»å‹æ¥æ”¶
			ch_source_rank = self._get_ch_source_rank()
			
			ch_features = []
			ch_tiers = []
			
			try:
				# ğŸ”¥ æ–°åŠŸèƒ½ï¼šç›´æ¥æ¥æ”¶List[Tuple[torch.Tensor, int]]æ ¼å¼
				if hasattr(self.node_comm, 'recv_tensor_tuple_list'):
					# ä½¿ç”¨ä¸“é—¨çš„æ–¹æ³•æ¥æ”¶tensor-tupleåˆ—è¡¨
					ch_data = self.node_comm.recv_tensor_tuple_list(
						src_rank=ch_source_rank,
						tag=100
					)
					
					if ch_data is not None:
						ch_features = [item[0] for item in ch_data]
						ch_tiers = [item[1] for item in ch_data]
						self.logger.debug(f"âœ… CHç‰¹å¾æ¥æ”¶æˆåŠŸ: {len(ch_data)}ä¸ªç‰¹å¾")
						self.update_comm_stats(True)
					else:
						self.logger.error("âŒ CHç‰¹å¾æ¥æ”¶å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
						self.update_comm_stats(False)
						ch_features, ch_tiers = self._fallback_recv_ch_features(ch_source_rank)
				
				elif hasattr(self.node_comm, 'recv_data'):
					# ä½¿ç”¨é€šç”¨çš„å¤æ‚æ•°æ®æ¥æ”¶æ–¹æ³•
					ch_data = self.node_comm.recv_data(
						src_rank=ch_source_rank,
						tag=100,
						reliable=True
					)
					
					if ch_data is not None and isinstance(ch_data, list):
						ch_features = [item[0] for item in ch_data]
						ch_tiers = [item[1] for item in ch_data]
						self.logger.debug(f"âœ… CHç‰¹å¾æ¥æ”¶æˆåŠŸ(é€šç”¨æ¨¡å¼): {len(ch_data)}ä¸ªç‰¹å¾")
						self.update_comm_stats(True)
					else:
						self.logger.error("âŒ CHç‰¹å¾æ¥æ”¶å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
						self.update_comm_stats(False)
						ch_features, ch_tiers = self._fallback_recv_ch_features(ch_source_rank)
				
				else:
					# å›é€€åˆ°åŸæœ‰æ¥æ”¶æ–¹å¼
					self.logger.warning("âš ï¸  ä½¿ç”¨åŸæœ‰æ¥æ”¶æ–¹å¼(ä¸æ”¯æŒå¤æ‚æ•°æ®ç±»å‹)")
					ch_features, ch_tiers = self._fallback_recv_ch_features(ch_source_rank)
			
			except Exception as e:
				self.logger.error(f"âŒ CHç‰¹å¾æ¥æ”¶å¼‚å¸¸: {e}")
				self.update_comm_stats(False)
				# å›é€€åˆ°åŸæœ‰æ¥æ”¶æ–¹å¼
				ch_features, ch_tiers = self._fallback_recv_ch_features(ch_source_rank)
			
			# æ¥æ”¶ç©ºé—´ç‰¹å¾(ä»èŠ‚ç‚¹1çš„GPU 3) - ä½¿ç”¨æ–°çš„å¤æ‚æ•°æ®ç±»å‹æ¥æ”¶
			spatial_source_rank = self._get_spatial_source_rank()
			
			spatial_features = []
			spatial_tiers = []
			
			try:
				# ğŸ”¥ æ–°åŠŸèƒ½ï¼šç›´æ¥æ¥æ”¶List[Tuple[torch.Tensor, int]]æ ¼å¼
				if hasattr(self.node_comm, 'recv_tensor_tuple_list'):
					# ä½¿ç”¨ä¸“é—¨çš„æ–¹æ³•æ¥æ”¶tensor-tupleåˆ—è¡¨
					spatial_data = self.node_comm.recv_tensor_tuple_list(
						src_rank=spatial_source_rank,
						tag=110  # å¯¹åº”SpatialFusionStageå‘é€çš„tag
					)
					
					if spatial_data is not None:
						spatial_features = [item[0] for item in spatial_data]
						spatial_tiers = [item[1] for item in spatial_data]
						self.logger.debug(f"âœ… ç©ºé—´ç‰¹å¾æ¥æ”¶æˆåŠŸ: {len(spatial_data)}ä¸ªç‰¹å¾")
						self.update_comm_stats(True)
					else:
						self.logger.error("âŒ ç©ºé—´ç‰¹å¾æ¥æ”¶å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
						self.update_comm_stats(False)
						spatial_features, spatial_tiers = self._fallback_recv_spatial_features(spatial_source_rank)
				
				elif hasattr(self.node_comm, 'recv_data'):
					# ä½¿ç”¨é€šç”¨çš„å¤æ‚æ•°æ®æ¥æ”¶æ–¹æ³•
					spatial_data = self.node_comm.recv_data(
						src_rank=spatial_source_rank,
						tag=110,
						reliable=True
					)
					
					if spatial_data is not None and isinstance(spatial_data, list):
						spatial_features = [item[0] for item in spatial_data]
						spatial_tiers = [item[1] for item in spatial_data]
						self.logger.debug(f"âœ… ç©ºé—´ç‰¹å¾æ¥æ”¶æˆåŠŸ(é€šç”¨æ¨¡å¼): {len(spatial_data)}ä¸ªç‰¹å¾")
						self.update_comm_stats(True)
					else:
						self.logger.error("âŒ ç©ºé—´ç‰¹å¾æ¥æ”¶å¤±è´¥ï¼Œå°è¯•å›é€€æ¨¡å¼")
						self.update_comm_stats(False)
						spatial_features, spatial_tiers = self._fallback_recv_spatial_features(spatial_source_rank)
				
				else:
					# å›é€€åˆ°åŸæœ‰æ¥æ”¶æ–¹å¼
					self.logger.warning("âš ï¸  ä½¿ç”¨åŸæœ‰æ¥æ”¶æ–¹å¼(ä¸æ”¯æŒå¤æ‚æ•°æ®ç±»å‹)")
					spatial_features, spatial_tiers = self._fallback_recv_spatial_features(spatial_source_rank)
			
			except Exception as e:
				self.logger.error(f"âŒ ç©ºé—´ç‰¹å¾æ¥æ”¶å¼‚å¸¸: {e}")
				self.update_comm_stats(False)
				# å›é€€åˆ°åŸæœ‰æ¥æ”¶æ–¹å¼
				spatial_features, spatial_tiers = self._fallback_recv_spatial_features(spatial_source_rank)
			
			# ç¡®ä¿ç‰¹å¾å’ŒtieråŒ¹é…
			ch_count = len(ch_features)
			spatial_count = len(spatial_features)
			if ch_count != spatial_count:
				print(f"Warning: CHç‰¹å¾({ch_count})å’Œç©ºé—´ç‰¹å¾({spatial_count})æ•°é‡ä¸åŒ¹é…")
			
			if ch_tiers != spatial_tiers:
				print(f"Warning: CHç‰¹å¾å’Œç©ºé—´ç‰¹å¾tierä¸åŒ¹é…")
			
			tiers = ch_tiers
		
		# å¤„ç†ç‰¹å¾
		fused_features = self.process(ch_features, spatial_features, tiers)
		
		# å°†èåˆç‰¹å¾å‘é€åˆ°å¤šå°ºåº¦èåˆé˜¶æ®µ(GPU 5)
		if self.node_comm:
			next_rank = self.node_comm.rank + 1
			
			# å‘é€featuresæ•°é‡
			count_tensor = torch.tensor([len(fused_features)], dtype=torch.long, device=self.device)
			self.node_comm.send_tensor(count_tensor, dst_rank=next_rank)
			
			# å‘é€æ¯ä¸ªèåˆç‰¹å¾
			for fused, tier in fused_features:
				# å‘é€èåˆç‰¹å¾
				self.node_comm.send_tensor(fused, dst_rank=next_rank)
				
				# å‘é€tierä¿¡æ¯
				tier_tensor = torch.tensor([tier], dtype=torch.long, device=self.device)
				self.node_comm.send_tensor(tier_tensor, dst_rank=next_rank)
		
		return fused_features
	
	def _fallback_recv_ch_features(self, ch_source_rank):
		"""å›é€€æ¥æ”¶æ–¹å¼ - å…¼å®¹åŸæœ‰é€šä¿¡æ–¹å¼"""
		ch_features = []
		ch_tiers = []
		
		try:
			# æ¥æ”¶featuresæ•°é‡
			count_tensor = self.node_comm.recv_tensor(
				src_rank=ch_source_rank,
				dtype=torch.long,
				device=self.device,
				reliable=False
			)
			
			if count_tensor is None:
				self.logger.error("æ— æ³•æ¥æ”¶CHç‰¹å¾æ•°é‡")
				return ch_features, ch_tiers
			
			ch_count = count_tensor.item()
			
			# æ¥æ”¶æ¯ä¸ªCHç‰¹å¾
			for i in range(ch_count):
				# æ¥æ”¶CHç‰¹å¾
				ch_feat = self.node_comm.recv_tensor(
					src_rank=ch_source_rank,
					device=self.device,
					reliable=False
				)
				
				# æ¥æ”¶tierä¿¡æ¯
				tier_tensor = self.node_comm.recv_tensor(
					src_rank=ch_source_rank,
					dtype=torch.long,
					device=self.device,
					reliable=False
				)
				
				if ch_feat is not None and tier_tensor is not None:
					ch_features.append(ch_feat)
					ch_tiers.append(tier_tensor.item())
				else:
					self.logger.warning(f"CHç‰¹å¾æˆ–tieræ¥æ”¶å¤±è´¥: ç¬¬{i}ä¸ª")
			
			self.logger.debug(f"âœ… CHç‰¹å¾æ¥æ”¶æˆåŠŸ(å›é€€æ¨¡å¼): {len(ch_features)}ä¸ªç‰¹å¾")
		
		except Exception as e:
			self.logger.error(f"âŒ å›é€€æ¥æ”¶ä¹Ÿå¤±è´¥: {e}")
		
		return ch_features, ch_tiers
	
	def _fallback_recv_spatial_features(self, spatial_source_rank):
		"""å›é€€æ¥æ”¶ç©ºé—´ç‰¹å¾ - å…¼å®¹åŸæœ‰é€šä¿¡æ–¹å¼"""
		spatial_features = []
		spatial_tiers = []
		
		try:
			# æ¥æ”¶featuresæ•°é‡
			count_tensor = self.node_comm.recv_tensor(
				src_rank=spatial_source_rank,
				dtype=torch.long,
				device=self.device,
				reliable=False
			)
			
			if count_tensor is None:
				self.logger.error("æ— æ³•æ¥æ”¶ç©ºé—´ç‰¹å¾æ•°é‡")
				return spatial_features, spatial_tiers
			
			spatial_count = count_tensor.item()
			
			# æ¥æ”¶æ¯ä¸ªç©ºé—´ç‰¹å¾
			for i in range(spatial_count):
				# æ¥æ”¶ç©ºé—´ç‰¹å¾
				spatial_feat = self.node_comm.recv_tensor(
					src_rank=spatial_source_rank,
					device=self.device,
					reliable=False
				)
				
				# æ¥æ”¶tierä¿¡æ¯
				tier_tensor = self.node_comm.recv_tensor(
					src_rank=spatial_source_rank,
					dtype=torch.long,
					device=self.device,
					reliable=False
				)
				
				if spatial_feat is not None and tier_tensor is not None:
					spatial_features.append(spatial_feat)
					spatial_tiers.append(tier_tensor.item())
				else:
					self.logger.warning(f"ç©ºé—´ç‰¹å¾æˆ–tieræ¥æ”¶å¤±è´¥: ç¬¬{i}ä¸ª")
			
			self.logger.debug(f"âœ… ç©ºé—´ç‰¹å¾æ¥æ”¶æˆåŠŸ(å›é€€æ¨¡å¼): {len(spatial_features)}ä¸ªç‰¹å¾")
		
		except Exception as e:
			self.logger.error(f"âŒ ç©ºé—´ç‰¹å¾å›é€€æ¥æ”¶ä¹Ÿå¤±è´¥: {e}")
		
		return spatial_features, spatial_tiers
	
	def _get_ch_source_rank(self):
		"""è·å–CHåˆ†æ”¯çš„æºrank"""
		# CHåˆ†æ”¯é€šå¸¸åœ¨å½“å‰rankçš„å‰ä¸€ä¸ªèŠ‚ç‚¹
		if hasattr(self.node_comm, 'node_ranks') and len(self.node_comm.node_ranks) > 0:
			# è®¡ç®—CHåˆ†æ”¯rank (èŠ‚ç‚¹1çš„GPU 2)
			return self.node_comm.node_ranks[0] + 2  # èŠ‚ç‚¹1çš„ç¬¬3ä¸ªGPU (index 2)
		else:
			# å›é€€è®¡ç®—
			return max(0, self.node_comm.rank - 2)
	
	def _get_spatial_source_rank(self):
		"""è·å–ç©ºé—´åˆ†æ”¯çš„æºrank"""
		# ç©ºé—´åˆ†æ”¯é€šå¸¸åœ¨å½“å‰rankçš„å‰ä¸€ä¸ªèŠ‚ç‚¹
		if hasattr(self.node_comm, 'node_ranks') and len(self.node_comm.node_ranks) > 0:
			# è®¡ç®—ç©ºé—´åˆ†æ”¯rank (èŠ‚ç‚¹1çš„GPU 3)
			return self.node_comm.node_ranks[0] + 3  # èŠ‚ç‚¹1çš„ç¬¬4ä¸ªGPU (index 3)
		else:
			# å›é€€è®¡ç®—
			return max(0, self.node_comm.rank - 1)
	
	def get_state_dict_prefix(self):
		"""è·å–å¸¦å‰ç¼€çš„å‚æ•°å­—å…¸"""
		# ä¿å­˜æ³¨æ„åŠ›èåˆå‚æ•°
		state_dict = {}
		for name, param in self.attention_fusion.state_dict().items():
			state_dict[f'attention_fusion.{name}'] = param
		return state_dict


# èŠ‚ç‚¹2 (GPU 5): å¤šå°ºåº¦èåˆ
class MultiscaleFusionStage(BaseStage):
	"""å¤šå°ºåº¦èåˆé˜¶æ®µ"""
	
	def __init__(self, model, device, node_comm=None, shared_components=None, config=None):
		super().__init__("MultiscaleFusionStage", device, node_comm)
		self.config = config or {}
		
		# æå–å¤šå°ºåº¦èåˆç»„ä»¶ - è¿™äº›é€šå¸¸æ¥è‡ªæ¨¡å‹ï¼Œä¸æ˜¯ç‹¬ç«‹åˆå§‹åŒ–çš„
		self.multiscale_fusion = model.multiscale_fusion
		
		# ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
		self.multiscale_fusion.to(device)
		
		# ç‰¹å¾ç¼“å­˜
		self.tier_features = {}
	
	def process(self, fused_features):
		"""æ‰§è¡Œå¤šå°ºåº¦èåˆ"""
		start_time = time.time()
		
		try:
			# æ›´æ–°tierç‰¹å¾å­—å…¸
			self.tier_features.clear()
			for fused, tier in fused_features:
				self.tier_features[tier] = fused
			
			# å¦‚æœåªæœ‰ä¸€ä¸ªtierï¼Œç›´æ¥è¿”å›
			if len(self.tier_features) == 1:
				tier = list(self.tier_features.keys())[0]
				result = self.tier_features[tier]
			elif len(self.tier_features) > 1:
				# æ‰§è¡Œå¤šå°ºåº¦èåˆ
				result = self.multiscale_fusion(self.tier_features)
			else:
				# æ²¡æœ‰ç‰¹å¾ï¼Œè¿”å›None
				return None
			
			self.compute_time += time.time() - start_time
			self.batch_count += 1
			
			return result
		
		except Exception as e:
			print(f"Multiscale fusion failed: {e}")
			return None
	
	def forward(self, fused_features=None):
		"""åŒæ­¥å‰å‘å¤„ç†"""
		if fused_features is None and self.node_comm:
			# ä»ç‰¹å¾èåˆé˜¶æ®µ(GPU 4)æ¥æ”¶æ•°æ®
			prev_rank = self.node_comm.rank - 1
			
			# æ¥æ”¶featuresæ•°é‡
			count_tensor = self.node_comm.recv_tensor(
				src_rank=prev_rank,
				dtype=torch.long,
				device=self.device
			)
			count = count_tensor.item()
			
			# æ¥æ”¶æ¯ä¸ªèåˆç‰¹å¾
			fused_features = []
			for i in range(count):
				# æ¥æ”¶èåˆç‰¹å¾
				fused = self.node_comm.recv_tensor(
					src_rank=prev_rank,
					device=self.device
				)
				
				# æ¥æ”¶tierä¿¡æ¯
				tier_tensor = self.node_comm.recv_tensor(
					src_rank=prev_rank,
					dtype=torch.long,
					device=self.device
				)
				
				fused_features.append((fused, tier_tensor.item()))
		
		# å¤„ç†ç‰¹å¾
		multiscale_result = self.process(fused_features)
		
		# å°†å¤šå°ºåº¦èåˆç»“æœå‘é€åˆ°åˆ†å‰²å¤´é˜¶æ®µ(GPU 6)
		if self.node_comm and multiscale_result is not None:
			next_rank = self.node_comm.rank + 1
			
			# å‘é€å¤šå°ºåº¦èåˆç»“æœ
			self.node_comm.send_tensor(multiscale_result, dst_rank=next_rank)
		
		return multiscale_result
	
	def get_state_dict_prefix(self):
		"""è·å–å¸¦å‰ç¼€çš„å‚æ•°å­—å…¸"""
		# ä¿å­˜å¤šå°ºåº¦èåˆå‚æ•°
		state_dict = {}
		for name, param in self.multiscale_fusion.state_dict().items():
			state_dict[f'multiscale_fusion.{name}'] = param
		return state_dict


# èŠ‚ç‚¹2 (GPU 6): åˆ†å‰²å¤´å’ŒæŸå¤±è®¡ç®—
class BackendStage(BaseStage):
	"""åç«¯å¤„ç†é˜¶æ®µ - å¤„ç†åˆ†å‰²å¤´å’ŒæŸå¤±è®¡ç®—"""
	
	def __init__(self, model, device, node_comm=None, shared_components=None, config=None):
		super().__init__("BackendStage", device, node_comm)
		self.config = config or {}
		
		# æå–åˆ†å‰²å¤´ç»„ä»¶
		self.seg_head_first = model.seg_head_first  # å¯èƒ½æ˜¯None
		self.seg_head_tail = model.seg_head_tail
		
		# åˆ›å»ºæŸå¤±å‡½æ•° - ä¼˜å…ˆä½¿ç”¨å…±äº«ç»„ä»¶
		self.criterion = None
		if shared_components and 'criterion' in shared_components:
			self.criterion = shared_components['criterion']
		
		else:
			from loss import CombinedLoss
			
			self.criterion = CombinedLoss()
		
		# ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
		if self.seg_head_first is not None:
			self.seg_head_first.to(device)
		self.seg_head_tail.to(device)
		self.criterion.to(device)
		
		# å½“å‰tier
		self.current_tier = None
	
	def set_tier(self, tier):
		"""è®¾ç½®å½“å‰tier"""
		self.current_tier = tier
	
	def _build_seg_head(self, in_c, ref):
		"""æ„å»ºåˆ†å‰²å¤´"""
		self.seg_head_first = nn.Conv3d(in_c, 32, 3, padding=1, bias=False)
		self.seg_head_first.to(ref.device, dtype=ref.dtype)
	
	def process(self, multiscale_result, labels=None):
		"""æ‰§è¡Œåˆ†å‰²å¤´å¤„ç†å’ŒæŸå¤±è®¡ç®—"""
		start_time = time.time()
		
		# åˆ†å‰²å¤´å¤„ç†
		if self.seg_head_first is None:
			# å»¶è¿Ÿæ„å»ºåˆ†å‰²å¤´
			self._build_seg_head(multiscale_result.shape[1], multiscale_result)
		
		# ç¡®ä¿åˆ†å‰²å¤´åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
		if next(self.seg_head_first.parameters()).device != multiscale_result.device:
			self.seg_head_first = self.seg_head_first.to(multiscale_result.device)
			self.seg_head_tail = self.seg_head_tail.to(multiscale_result.device)
		
		# æ‰§è¡Œåˆ†å‰²
		logits = self.seg_head_tail(self.seg_head_first(multiscale_result))
		
		# è®¡ç®—æŸå¤±(å¦‚æœæœ‰æ ‡ç­¾)
		loss = None
		if labels is not None and self.training:
			loss = self.criterion(logits, labels)
		
		self.compute_time += time.time() - start_time
		self.batch_count += 1
		
		return logits, loss
	
	def forward(self, multiscale_result=None, labels=None):
		"""åŒæ­¥å‰å‘å¤„ç†"""
		if multiscale_result is None and self.node_comm:
			# ä»å¤šå°ºåº¦èåˆé˜¶æ®µ(GPU 5)æ¥æ”¶æ•°æ®
			prev_rank = self.node_comm.rank - 1
			
			# æ¥æ”¶å¤šå°ºåº¦èåˆç»“æœ
			multiscale_result = self.node_comm.recv_tensor(
				src_rank=prev_rank,
				device=self.device
			)
		
		# å¤„ç†ç‰¹å¾
		logits, loss = self.process(multiscale_result, labels)
		
		return logits, loss
	
	def get_state_dict_prefix(self):
		"""è·å–å¸¦å‰ç¼€çš„å‚æ•°å­—å…¸"""
		# ä¿å­˜åˆ†å‰²å¤´å‚æ•°
		state_dict = {}
		if self.seg_head_first is not None:
			for name, param in self.seg_head_first.state_dict().items():
				state_dict[f'seg_head_first.{name}'] = param
		for name, param in self.seg_head_tail.state_dict().items():
			state_dict[f'seg_head_tail.{name}'] = param
		return state_dict


# åœ¨ scripts/distributed/stages.py æ–‡ä»¶æœ«å°¾æ·»åŠ ä»¥ä¸‹å‡½æ•°

def create_pipeline_stages(config, node_comm=None):
	"""
	åˆ›å»ºæµæ°´çº¿é˜¶æ®µçš„å·¥å‚å‡½æ•°

	å‚æ•°:
		config: é…ç½®å­—å…¸
		node_comm: èŠ‚ç‚¹é€šä¿¡ç®¡ç†å™¨

	è¿”å›:
		é˜¶æ®µå­—å…¸
	"""
	import torch
	from models import create_vessel_segmenter
	
	# è·å–å½“å‰rankå’Œè®¾å¤‡
	rank = torch.distributed.get_rank() if torch.distributed.is_initialized() else 0
	device = torch.cuda.current_device()
	
	# åˆ›å»ºå®Œæ•´æ¨¡å‹ï¼ˆç”¨äºæå–ç»„ä»¶ï¼‰
	full_model = create_vessel_segmenter(config)
	
	stages = {}
	
	# æ ¹æ®rankåˆ›å»ºç›¸åº”çš„é˜¶æ®µ - ä½¿ç”¨æ­£ç¡®çš„ç±»å
	if rank == 0:  # èŠ‚ç‚¹1, GPU 0 - é¢„å¤„ç†
		stages['preprocessing'] = FrontendStage(  # ä½¿ç”¨ç°æœ‰çš„FrontendStage
			full_model, device, node_comm, config=config
		)
	elif rank == 1:  # èŠ‚ç‚¹1, GPU 1 - é‡‡æ ·è°ƒåº¦
		stages['patch_scheduling'] = PatchSchedulingStage(
			full_model, device, node_comm, config=config
		)
	elif rank == 2:  # èŠ‚ç‚¹1, GPU 2 - CHåˆ†æ”¯
		stages['ch_branch'] = CHProcessingStage(
			full_model, device, node_comm, config=config
		)
	elif rank == 3:  # èŠ‚ç‚¹1, GPU 3 - ç©ºé—´åˆ†æ”¯
		stages['spatial_branch'] = SpatialFusionStage(
			full_model, device, node_comm, config=config
		)
	elif rank == 4:  # èŠ‚ç‚¹2, GPU 4 - ç‰¹å¾èåˆ
		stages['feature_fusion'] = FeatureFusionStage(
			full_model, device, node_comm, config=config
		)
	elif rank == 5:  # èŠ‚ç‚¹2, GPU 5 - å¤šå°ºåº¦èåˆ
		stages['multiscale_fusion'] = MultiscaleFusionStage(
			full_model, device, node_comm, config=config
		)
	elif rank == 6:  # èŠ‚ç‚¹2, GPU 6 - åˆ†å‰²å¤´
		stages['segmentation_head'] = BackendStage(  # ä½¿ç”¨ç°æœ‰çš„BackendStage
			full_model, device, node_comm, config=config
		)
	
	stages[7] = lambda model, device: DummyStage(model, device, node_comm, config)
	
	return stages


class SegmentationHeadStage(BaseStage):
	"""åˆ†å‰²å¤´é˜¶æ®µ"""
	
	def __init__(self, model, device, node_comm=None, shared_components=None, config=None):
		super().__init__("SegmentationHeadStage", device, node_comm)
		self.config = config or {}
		
		# æå–åˆ†å‰²å¤´ç»„ä»¶
		self.seg_head_first = model.seg_head_first
		self.seg_head_tail = model.seg_head_tail
		self.final_activation = model.final_activation
		
		# ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
		if self.seg_head_first is not None:
			self.seg_head_first.to(device)
		self.seg_head_tail.to(device)
		self.final_activation.to(device)
	
	def process(self, multiscale_features):
		"""å¤„ç†å¤šå°ºåº¦ç‰¹å¾ï¼Œè¾“å‡ºæœ€ç»ˆåˆ†å‰²ç»“æœ"""
		start_time = time.time()
		
		try:
			# å¦‚æœseg_head_firstå°šæœªæ„å»ºï¼Œåˆ™å»¶è¿Ÿæ„å»º
			if self.seg_head_first is None:
				in_channels = multiscale_features.shape[1]
				self.seg_head_first = torch.nn.Conv3d(
					in_channels, 32, 3, padding=1, bias=False
				).to(self.device)
			
			# åˆ†å‰²å¤´å¤„ç†
			x = self.seg_head_first(multiscale_features)
			x = self.seg_head_tail(x)
			output = self.final_activation(x)
			
			self.compute_time += time.time() - start_time
			self.batch_count += 1
			
			return output
		
		except Exception as e:
			print(f"Segmentation head processing failed: {e}")
			return None
	
	def forward(self, multiscale_features=None):
		"""åŒæ­¥å‰å‘å¤„ç†"""
		if multiscale_features is None and self.node_comm:
			# ä»å¤šå°ºåº¦èåˆé˜¶æ®µæ¥æ”¶æ•°æ®
			prev_rank = self.node_comm.rank - 1
			
			# æ¥æ”¶å¤šå°ºåº¦ç‰¹å¾
			multiscale_features = self.node_comm.recv_tensor(
				src_rank=prev_rank,
				device=self.device
			)
		
		# å¤„ç†ç‰¹å¾
		output = self.process(multiscale_features)
		
		return output
	
	def get_state_dict_prefix(self):
		"""è·å–å¸¦å‰ç¼€çš„å‚æ•°å­—å…¸"""
		state_dict = {}
		
		if self.seg_head_first is not None:
			for name, param in self.seg_head_first.state_dict().items():
				state_dict[f'seg_head_first.{name}'] = param
		
		for name, param in self.seg_head_tail.state_dict().items():
			state_dict[f'seg_head_tail.{name}'] = param
		
		for name, param in self.final_activation.state_dict().items():
			state_dict[f'final_activation.{name}'] = param
		
		return state_dict