#!/usr/bin/env python3
"""
é›†æˆåŒ–åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿ - è‚è„è¡€ç®¡åˆ†å‰²
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰distributedç›®å½•ä¸‹çš„åŠŸèƒ½ç»„ä»¶
"""

import os
import sys
import time
import argparse
import yaml
import logging
import signal
# åœ¨ç°æœ‰å¯¼å…¥ä¸­æ·»åŠ 
from tqdm import tqdm
import traceback
import copy
from pathlib import Path
from typing import Dict, Optional, Any, List, Tuple
from dataclasses import dataclass

import torch
import torch.distributed as dist
import torch.nn.functional as F
from torch.utils.data import DataLoader, DistributedSampler

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from data import LiverVesselDataset
from models import create_vessel_segmenter
from loss.combined_loss import CombinedLoss

# åˆ†å¸ƒå¼ç»„ä»¶å¯¼å…¥
from scripts.distributed.cross_node_pipeline import create_pipeline
from scripts.distributed.node_communicator import NodeCommunicator
from scripts.distributed.stages import create_pipeline_stages
from scripts.distributed.evaluation import EvaluationManager

# å¯é æ€§ç»„ä»¶å¯¼å…¥ - ä½¿ç”¨æ­£ç¡®çš„ç±»å
from scripts.distributed.reliability.checkpoint_manager import DistributedCheckpointManager
from scripts.distributed.reliability.parameter_synchronizer import ParameterSyncManager
from scripts.distributed.reliability.communication_reliability import EnhancedNodeCommunicator
from scripts.distributed.reliability.gradient_backoff import GradientBackoffHandler

# ç›‘æ§ç»„ä»¶å¯¼å…¥ - ä½¿ç”¨æ­£ç¡®çš„ç±»å
from scripts.distributed.monitoring.error_recovery import ErrorRecoverySystem

# å·¥å…·ç»„ä»¶å¯¼å…¥
from utils.component_factory import ComponentFactory
from utils import Logger

# é…ç½®æ—¥å¿—
logging.basicConfig(
	level=logging.INFO,
	format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class TrainingState:
	"""è®­ç»ƒçŠ¶æ€è·Ÿè¸ª"""
	epoch: int = 0
	batch: int = 0
	global_step: int = 0
	best_dice: float = 0.0
	best_loss: float = float('inf')
	training_time: float = 0.0
	last_checkpoint_time: float = 0.0


class ModelMerger:
	"""æ¨¡å‹åˆå¹¶å™¨ - å°†7ä¸ªGPUçš„checkpointsåˆå¹¶ä¸ºå®Œæ•´æ¨¡å‹"""
	
	def __init__(self, world_size: int, model_config: Dict, export_config: Dict, device: torch.device):
		self.world_size = world_size
		self.model_config = model_config
		self.export_config = export_config
		self.device = device
		self.logger = logging.getLogger(__name__)
	
	def merge_pipeline_checkpoints(self, checkpoint_paths: Dict[int, str]) -> torch.nn.Module:
		"""
		ä»7ä¸ªstageçš„checkpointsåˆå¹¶ä¸ºå®Œæ•´VesselSegmenter

		Args:
			checkpoint_paths: {rank: checkpoint_path} æ˜ å°„

		Returns:
			å®Œæ•´çš„VesselSegmenteræ¨¡å‹
		"""
		self.logger.info("å¼€å§‹åˆå¹¶æµæ°´çº¿æ¨¡å‹...")
		
		# 1. åˆ›å»ºå®Œæ•´æ¨¡å‹å®ä¾‹
		full_model = create_vessel_segmenter(self.model_config)
		full_model.to(self.device)
		
		# 2. é€ä¸ªåŠ è½½å„stageçš„å‚æ•°
		stage_mapping = {
			0: 'preprocessor',
			1: 'tier_sampler',
			2: 'ch_branch',
			3: 'spatial_branch',
			4: 'feature_fusion',
			5: 'multiscale_fusion',
			6: 'seg_head'
		}
		
		for rank, checkpoint_path in checkpoint_paths.items():
			if not Path(checkpoint_path).exists():
				self.logger.warning(f"Checkpointä¸å­˜åœ¨: {checkpoint_path}")
				continue
			
			try:
				checkpoint = torch.load(checkpoint_path, map_location=self.device)
				stage_state_dict = checkpoint.get('model_state_dict', {})
				
				stage_name = stage_mapping.get(rank)
				if stage_name and hasattr(full_model, stage_name):
					# åŠ è½½å¯¹åº”stageçš„å‚æ•°
					stage_module = getattr(full_model, stage_name)
					if stage_module is not None:
						stage_module.load_state_dict(stage_state_dict, strict=False)
						self.logger.info(f"æˆåŠŸåŠ è½½rank {rank} ({stage_name}) çš„å‚æ•°")
			
			except Exception as e:
				self.logger.error(f"è¯»å–checkpointå¤±è´¥ {checkpoint_path}: {e}")
				continue
		
		# 3. éªŒè¯æ¨¡å‹å®Œæ•´æ€§
		if self._validate_merged_model(full_model):
			self.logger.info("æ¨¡å‹åˆå¹¶å®Œæˆï¼ŒéªŒè¯é€šè¿‡")
		else:
			self.logger.warning("æ¨¡å‹åˆå¹¶å®Œæˆï¼Œä½†éªŒè¯å‘ç°é—®é¢˜")
		
		return full_model
	
	def _validate_merged_model(self, model: torch.nn.Module) -> bool:
		"""éªŒè¯åˆå¹¶åæ¨¡å‹çš„å®Œæ•´æ€§"""
		try:
			# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ä»¥å‰å‘ä¼ æ’­
			model.eval()
			dummy_input = torch.randn(1, 1, 64, 64, 64, device=self.device)
			
			with torch.no_grad():
				output = model(dummy_input)
			
			if output is not None and output.shape[0] == 1:
				return True
			else:
				self.logger.error("æ¨¡å‹è¾“å‡ºæ ¼å¼ä¸æ­£ç¡®")
				return False
		
		except Exception as e:
			self.logger.error(f"æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
			return False
	
	def export_for_deployment(self, model: torch.nn.Module, export_path: str) -> bool:
		"""
		å¯¼å‡ºä¸ºéƒ¨ç½²ç”¨çš„æ¨¡å‹æ–‡ä»¶

		Args:
			model: è¦å¯¼å‡ºçš„æ¨¡å‹
			export_path: å¯¼å‡ºè·¯å¾„

		Returns:
			æ˜¯å¦å¯¼å‡ºæˆåŠŸ
		"""
		try:
			export_path = Path(export_path)
			export_path.parent.mkdir(parents=True, exist_ok=True)
			
			# å¯¼å‡ºPyTorchæ¨¡å‹
			formats = self.export_config.get('export_formats', ['pth'])
			
			if 'pth' in formats:
				pth_path = export_path.with_suffix('.pth')
				torch.save({
					'model_state_dict': model.state_dict(),
					'model_config': self.model_config,
					'export_time': time.time(),
					'pytorch_version': torch.__version__
				}, pth_path)
				self.logger.info(f"PyTorchæ¨¡å‹å·²å¯¼å‡º: {pth_path}")
			
			if 'onnx' in formats:
				import torch.onnx
				onnx_path = export_path.with_suffix('.onnx')
				dummy_input = torch.randn(1, 1, 64, 64, 64, device=self.device)
				
				torch.onnx.export(
					model,
					dummy_input,
					onnx_path,
					export_params=True,
					opset_version=11,
					do_constant_folding=True,
					input_names=['input'],
					output_names=['output']
				)
				self.logger.info(f"ONNXæ¨¡å‹å·²å¯¼å‡º: {onnx_path}")
			
			return True
		
		except Exception as e:
			self.logger.error(f"æ¨¡å‹å¯¼å‡ºå¤±è´¥: {e}")
			return False
	
	def collect_all_checkpoints(self, checkpoint_dir: str, epoch: int) -> Dict[int, str]:
		"""æ”¶é›†æ‰€æœ‰rankçš„checkpointè·¯å¾„"""
		checkpoint_paths = {}
		
		for rank in range(self.world_size):
			checkpoint_name = f"checkpoint_epoch_{epoch}_rank_{rank}.pt"
			checkpoint_path = Path(checkpoint_dir) / checkpoint_name
			
			if checkpoint_path.exists():
				checkpoint_paths[rank] = str(checkpoint_path)
			else:
				self.logger.warning(f"Checkpointä¸å­˜åœ¨: {checkpoint_path}")
		
		return checkpoint_paths


class IntegratedPipelineTrainer:
	"""é›†æˆåŒ–æµæ°´çº¿è®­ç»ƒå™¨ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰åˆ†å¸ƒå¼ç»„ä»¶"""
	
	def __init__(self, rank: int, world_size: int, local_rank: int, config: Dict, args):
		"""
		åˆå§‹åŒ–é›†æˆåŒ–æµæ°´çº¿è®­ç»ƒå™¨

		Args:
			rank: å…¨å±€è¿›ç¨‹æ’å
			world_size: æ€»è¿›ç¨‹æ•°
			local_rank: æœ¬åœ°GPUç¼–å·
			config: é…ç½®å­—å…¸
			args: å‘½ä»¤è¡Œå‚æ•°
		"""
		self.rank = rank
		self.world_size = world_size
		self.local_rank = local_rank
		self.args = args
		self.device = torch.cuda.current_device()
		
		# é…ç½®éªŒè¯å’Œé»˜è®¤å€¼åˆå¹¶
		self.config = self._validate_and_merge_config(config)
		
		# è®­ç»ƒçŠ¶æ€
		self.training_state = TrainingState()
		
		# è®¾ç½®æµæ°´çº¿é…ç½®
		self.pipeline_config = {
			0: {'stage': 'preprocessing', 'next_ranks': [1], 'prev_ranks': []},
			1: {'stage': 'patch_scheduling', 'next_ranks': [2, 3], 'prev_ranks': [0]},
			2: {'stage': 'ch_branch', 'next_ranks': [4], 'prev_ranks': [1]},
			3: {'stage': 'spatial_branch', 'next_ranks': [4], 'prev_ranks': [1]},
			4: {'stage': 'feature_fusion', 'next_ranks': [5], 'prev_ranks': [2, 3]},
			5: {'stage': 'multiscale_fusion', 'next_ranks': [6], 'prev_ranks': [4]},
			6: {'stage': 'segmentation_head', 'next_ranks': [], 'prev_ranks': [5]},
			7: {'stage': 'dummy_stage', 'next_ranks': [], 'prev_ranks': []}
		}
		
		self.stage_config = self.pipeline_config[rank]
		self.next_ranks = self.stage_config['next_ranks']
		self.prev_ranks = self.stage_config['prev_ranks']
		
		# åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
		self._setup_basic_components()
		self._setup_enhanced_components()
		self._setup_evaluation_and_logging()
		self._setup_model_merger()
		
		logger.info(f"Rank {rank}: IntegratedPipelineTraineråˆå§‹åŒ–å®Œæˆ")
	
	def _validate_and_merge_config(self, config: Dict) -> Dict:
		"""éªŒè¯é…ç½®å¹¶ä¸é»˜è®¤å€¼åˆå¹¶"""
		
		# æä¾›é»˜è®¤çš„åˆ†å¸ƒå¼é…ç½®
		default_distributed_config = {
			'communication': {
				'enable_reliability': True,
				'max_chunk_size': 50 * 1024 * 1024,  # 50MB
				'compression_level': 6,
				'send_timeout': 30.0,
				'recv_timeout': 60.0,
				'enable_ack': True,
				'ack_timeout': 5.0,
				'node_count': 2
			},
			'parameter_sync': {
				'enable_sync': True,
				'sync_frequency': 'batch',
				'enable_version_control': True
			},
			'gradient_reliability': {
				'enable_gradient_recovery': True,
				'max_recovery_attempts': 3,
				'enable_double_precision_fallback': True,
				'enable_gradient_checkpointing': False
			},
			'health_monitoring': {
				'enable_monitoring': True,
				'check_interval': 30,
				'gpu_temp_threshold': 85,
				'memory_threshold': 0.9,
				'cpu_threshold': 0.8,
				'network_latency_threshold': 100
			},
			'checkpoint': {
				'enable_compression': True,
				'compression_level': 6,
				'save_metadata': True,
				'checkpoint_version': "1.0.0"
			}
		}
		
		default_eval_config = {
			'eval_full_interval': 10,
			'eval_quick_interval': 2,
			'quick_samples': 5,
			'group_by_tier': True,
			'feature_mmap_enabled': True,
			'feature_mmap_dir': 'eval_tier_features',
			'clear_cache_interval': 3
		}
		
		default_logging_config = {
			'enable_tensorboard': True,
			'log_dir': './logs',
			'experiment_name': 'liver_vessel_pipeline',
			'log_level': 'INFO',
			'log_interval': 10
		}
		
		default_export_config = {
			'enable_export': True,
			'export_formats': ['pth'],
			'validate_merged_model': True,
			'export_dir': './exported_models'
		}
		
		# æ·±åº¦åˆå¹¶é…ç½®
		merged_config = copy.deepcopy(config)
		
		if 'distributed' not in merged_config:
			merged_config['distributed'] = {}
		merged_config['distributed'] = self._deep_merge(
			default_distributed_config,
			merged_config['distributed']
		)
		
		if 'evaluation' not in merged_config:
			merged_config['evaluation'] = default_eval_config
		else:
			merged_config['evaluation'] = self._deep_merge(
				default_eval_config,
				merged_config['evaluation']
			)
		
		if 'logging' not in merged_config:
			merged_config['logging'] = default_logging_config
		else:
			merged_config['logging'] = self._deep_merge(
				default_logging_config,
				merged_config['logging']
			)
		
		if 'model_export' not in merged_config:
			merged_config['model_export'] = default_export_config
		else:
			merged_config['model_export'] = self._deep_merge(
				default_export_config,
				merged_config['model_export']
			)
		
		# éªŒè¯å…³é”®é…ç½®é¡¹
		self._validate_config(merged_config)
		
		return merged_config
	
	def _deep_merge(self, default_dict: Dict, user_dict: Dict) -> Dict:
		"""é€’å½’åˆå¹¶å­—å…¸"""
		result = copy.deepcopy(default_dict)
		
		for key, value in user_dict.items():
			if key in result and isinstance(result[key], dict) and isinstance(value, dict):
				result[key] = self._deep_merge(result[key], value)
			else:
				result[key] = value
		
		return result
	
	def _validate_config(self, config: Dict):
		"""éªŒè¯é…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§"""
		required_sections = ['model', 'data', 'loss']
		for section in required_sections:
			if section not in config:
				raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„section: {section}")
		
		# éªŒè¯distributedé…ç½®çš„åˆç†æ€§
		distributed_config = config.get('distributed', {})
		comm_config = distributed_config.get('communication', {})
		
		send_timeout = comm_config.get('send_timeout', 30)
		if send_timeout < 10:
			logger.warning("send_timeoutè®¾ç½®è¿‡å°ï¼Œå¯èƒ½å¯¼è‡´ä¼ è¾“å¤±è´¥")
		
		recv_timeout = comm_config.get('recv_timeout', 60)
		if recv_timeout < send_timeout:
			logger.warning("recv_timeoutåº”è¯¥å¤§äºsend_timeout")
	
	def _setup_basic_components(self):
		"""è®¾ç½®åŸºç¡€ç»„ä»¶"""
		
		# åˆ›å»ºèŠ‚ç‚¹é€šä¿¡å™¨å¹¶å‡çº§ä¸ºå¢å¼ºç‰ˆæœ¬
		from scripts.distributed.reliability.communication_reliability import upgrade_node_communicator
		
		comm_config = self.config.get('distributed', {}).get('communication', {})
		node_count = comm_config.get('node_count', 2)
		
		# åˆ›å»ºåŸå§‹é€šä¿¡å™¨
		original_node_comm = NodeCommunicator(
			world_size=self.world_size,
			rank=self.rank,
			local_rank=self.local_rank,
			node_rank=self.rank // (self.world_size // node_count),
			node_count=node_count
		)
		
		# å‡çº§ä¸ºå¢å¼ºç‰ˆæœ¬
		self.node_comm = upgrade_node_communicator(original_node_comm)
		
		# è®°å½•é€šä¿¡å™¨ç‰ˆæœ¬
		logger.info(f"ğŸ”— é€šä¿¡å™¨åˆå§‹åŒ–å®Œæˆ - Rank {self.rank}:")
		logger.info(f"  - å¯é ä¼ è¾“: âœ… å¯ç”¨")
		logger.info(f"  - å¤æ‚æ•°æ®ç±»å‹: âœ… æ”¯æŒ")
		logger.info(f"  - å›é€€æ¨¡å¼: âœ… æ”¯æŒ")
		
		# åˆ›å»ºå®Œæ•´æ¨¡å‹ï¼ˆç”¨äºæå–ç»„ä»¶ï¼‰
		self.full_model = create_vessel_segmenter(self.config)
		
		# åˆ›å»ºå½“å‰rankçš„stage
		self.stage = self._create_stage(self.full_model)
		
		# æ”¶é›†æ‰€æœ‰å‚æ•°ï¼ˆç”¨äºä¼˜åŒ–å™¨ï¼‰
		self.all_params = list(self.full_model.parameters())
		
		# åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä»…rank 0ï¼‰
		self._setup_dataloader()
		
		# åˆ›å»ºè®­ç»ƒç»„ä»¶
		self._setup_training_components()
	
	def _create_stage(self, full_model):
		"""åˆ›å»ºå½“å‰rankå¯¹åº”çš„stage"""
		# ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºstage
		stage_dict = create_pipeline_stages(
			config=self.config,
			node_comm=self.node_comm
		)
		return stage_dict.get(self.stage_config['stage'])
	
	def _setup_dataloader(self):
		"""è®¾ç½®æ•°æ®åŠ è½½å™¨"""
		if self.rank == 0:
			logger.info("ğŸ“Š å¼€å§‹åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
			logger.info("ğŸ” æ­£åœ¨åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
			
			data_config = self.config.get('data', {})
			train_dataset = LiverVesselDataset(
				image_dir=self.args.image_dir,
				label_dir=self.args.label_dir,
				max_cases=data_config.get('max_cases', 5),
				random_sampling=data_config.get('random_sampling', True),
				enable_smart_sampling=data_config.get('enable_smart_sampling', True),
				
			)
			
			self.train_loader = DataLoader(
				train_dataset,
				batch_size=self.args.batch_size,
				shuffle=True,
				num_workers=self.args.num_workers,
				pin_memory=True,
				drop_last=True
			)
			
			# åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
			val_dataset = LiverVesselDataset(
				image_dir=self.args.image_dir,
				label_dir=self.args.label_dir,
				max_cases=data_config.get('val_max_cases', 20),
				random_sampling=False,
				enable_smart_sampling=False,
			)
			
			self.val_loader = DataLoader(
				val_dataset,
				batch_size=1,  # éªŒè¯æ—¶ä½¿ç”¨batch_size=1
				shuffle=False,
				num_workers=2,
				pin_memory=True
			)
			
			self.labels_cache = {}  # ç¼“å­˜labelsç”¨äºrank 6
		else:
			self.train_loader = None
			self.val_loader = None
	
	def _setup_training_components(self):
		"""è®¾ç½®è®­ç»ƒç»„ä»¶"""
		# åˆ›å»ºä¼˜åŒ–å™¨
		self.optimizer = torch.optim.AdamW(
			self.all_params,
			lr=self.args.lr,
			weight_decay=1e-4
		)
		
		# åˆ›å»ºæŸå¤±å‡½æ•°ï¼ˆä»…rank 6ï¼‰
		if self.rank == 6:
			self.loss_fn = CombinedLoss()
		
		# å­¦ä¹ ç‡è°ƒåº¦å™¨
		self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
			self.optimizer, T_max=self.args.epochs
		)
		
		# æ··åˆç²¾åº¦
		self.scaler = torch.cuda.amp.GradScaler() if self.args.amp else None




	def _setup_enhanced_components(self):
		"""è®¾ç½®å¢å¼ºç»„ä»¶"""
		distributed_config = self.config.get('distributed', {})
		
		# 1. é€šä¿¡å¯é æ€§å¢å¼º
		comm_config = distributed_config.get('communication', {})
		if comm_config.get('enable_reliability', True):
			self.enhanced_comm = EnhancedNodeCommunicator(self.node_comm)
			# æ›¿æ¢åŸæœ‰é€šä¿¡å™¨
			self.node_comm = self.enhanced_comm
		
		# 2. å‚æ•°åŒæ­¥ç®¡ç†
		sync_config = distributed_config.get('parameter_sync', {})
		if sync_config.get('enable_sync', True):
			self.param_sync = ParameterSyncManager(
				model=self.stage,
				rank=self.rank,
				world_size=self.world_size,
				device=self.device
			)
		else:
			self.param_sync = None
		
		# 3. æ¢¯åº¦å¯é æ€§ç®¡ç†
		grad_config = distributed_config.get('gradient_reliability', {})
		if grad_config.get('enable_gradient_recovery', True):
			self.gradient_manager = GradientBackoffHandler(
				model=self.stage,
				save_dir=self.args.output_dir
			)
		else:
			self.gradient_manager = None
		
		# 4. ç³»ç»Ÿå¥åº·ç›‘æ§
		health_config = distributed_config.get('health_monitoring', {})
		if health_config.get('enable_monitoring', True):
			self.health_monitor = ErrorRecoverySystem(
				rank=self.rank,
				device=self.device
			)
		else:
			self.health_monitor = None
		
		# 5. åˆ†å¸ƒå¼æ£€æŸ¥ç‚¹ç®¡ç†å™¨
		checkpoint_config = distributed_config.get('checkpoint', {})
		self.distributed_checkpoint = DistributedCheckpointManager(
			rank=self.rank,
			world_size=self.world_size,
			base_dir=self.args.output_dir
		)
	
	
	
	
	def _setup_evaluation_and_logging(self):
		"""è®¾ç½®è¯„ä¼°å’Œæ—¥å¿—ç»„ä»¶"""
		
		# è¯„ä¼°ç®¡ç†å™¨ï¼ˆæ‰€æœ‰rankéƒ½éœ€è¦ï¼Œä½†åªæœ‰rank 6çœŸæ­£æ‰§è¡Œï¼‰
		eval_config = self.config.get('evaluation', {})
		self.evaluator = EvaluationManager(
			config=eval_config,
			logger=None  # æš‚æ—¶ä¸ä¼ loggerï¼Œåé¢ä¼šè®¾ç½®
		)
		
		# æ—¥å¿—è®°å½•å™¨ï¼ˆåªåœ¨rank 0åˆå§‹åŒ–ï¼‰
		if self.rank == 0:
			log_config = self.config.get('logging', {})
			
			self.logger = Logger(
				log_dir=log_config.get('log_dir', self.args.output_dir),
				experiment_name=log_config.get('experiment_name', 'liver_vessel_pipeline')
			)
			
			# å°†loggerä¼ é€’ç»™è¯„ä¼°å™¨
			self.evaluator.logger = self.logger
			
			logger.info("Tensorboardæ—¥å¿—å™¨åˆå§‹åŒ–æˆåŠŸ")
		else:
			self.logger = None
	
	def _setup_model_merger(self):
		"""è®¾ç½®æ¨¡å‹åˆå¹¶å™¨"""
		export_config = self.config.get('model_export', {})
		self.model_merger = ModelMerger(
			world_size=self.world_size,
			model_config=self.config.get('model', {}),
			export_config=export_config,
			device=self.device
		)
	
	def train_epoch(self, epoch: int) -> Dict[str, float]:
		"""
		è®­ç»ƒä¸€ä¸ªepoch

		Args:
			epoch: å½“å‰epoch

		Returns:
			è®­ç»ƒæŒ‡æ ‡å­—å…¸
		"""
		self.training_state.epoch = epoch
		
		# Epochå¼€å§‹å‰çš„åŒæ­¥å’Œæ£€æŸ¥
		self._pre_epoch_sync_and_check(epoch)
		
		# æ‰§è¡Œå¢å¼ºè®­ç»ƒå¾ªç¯
		epoch_metrics = self._execute_enhanced_training_loop(epoch)
		
		# Epochç»“æŸåçš„åŒæ­¥å’Œä¿å­˜
		self._post_epoch_sync_and_save(epoch, epoch_metrics)
		
		return epoch_metrics
	
	def _pre_epoch_sync_and_check(self, epoch: int):
		"""Epochå¼€å§‹å‰çš„åŒæ­¥å’Œæ£€æŸ¥"""
		
		# å‚æ•°åŒæ­¥
		if self.param_sync:
			if not self.param_sync.sync_on_epoch_start():
				logger.warning(f"Epoch {epoch} å¼€å§‹å‚æ•°åŒæ­¥å¤±è´¥")
		
		# å¥åº·æ£€æŸ¥
		if self.health_monitor:
			self.health_monitor.start_monitoring()
			if not self.health_monitor.check_system_health():
				logger.warning(f"Rank {self.rank} å¥åº·æ£€æŸ¥å‘ç°é—®é¢˜")
		
		
		
		# å…¨å±€åŒæ­¥ç‚¹
		dist.barrier()
		
		if self.rank == 0:
			logger.info(f"Epoch {epoch} å¼€å§‹è®­ç»ƒ")
	
	def _execute_enhanced_training_loop(self, epoch: int) -> Dict[str, float]:
		"""æ‰§è¡Œå¢å¼ºç‰ˆè®­ç»ƒå¾ªç¯ - å¸¦è¿›åº¦æ¡"""
		
		if self.rank == 7:
			# DummyStageä»€ä¹ˆéƒ½ä¸åšï¼Œåªæ˜¯å ç”¨GPU
			self.stage.train()
			time.sleep(1)  # æ¨¡æ‹Ÿä¸€äº›å·¥ä½œ
			return {
				'loss': 0.0,
				'epoch_time': 1.0,
				'batch_count': 0
			}
		
		
		
		# è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
		self.stage.train()
		
		total_loss = 0.0
		batch_count = 0
		epoch_start_time = time.time()
		
		# âœ… æ·»åŠ Batchçº§åˆ«è¿›åº¦æ¡
		if self.rank == 0 and self.train_loader:
			# Rank 0: æ˜¾ç¤ºçœŸå®çš„æ•°æ®åŠ è½½è¿›åº¦
			batch_pbar = tqdm(
				enumerate(self.train_loader),
				desc=f"ğŸ“¦ Epoch {epoch} Batches",
				total=len(self.train_loader),
				leave=False,
				ncols=120,
				colour='blue'
			)
			data_iterator = batch_pbar
		else:
			# å…¶ä»–rank: ä¼°ç®—batchæ•°é‡
			estimated_batches = len(self.train_loader) if hasattr(self, 'train_loader') and self.train_loader else 100
			batch_pbar = tqdm(
				enumerate([(i, None) for i in range(estimated_batches)]),
				desc=f"ğŸ”§ Rank {self.rank} Epoch {epoch}",
				total=estimated_batches,
				leave=False,
				ncols=100,
				colour='yellow'
			)
			data_iterator = batch_pbar
		
		try:
			for batch_idx, batch in data_iterator:
				batch_start_time = time.time()
				
				try:
					# æ‰¹æ¬¡å¼€å§‹å‰çš„æ£€æŸ¥å’ŒåŒæ­¥
					if self.health_monitor:
						if not self.health_monitor.check_system_health():
							self._handle_health_issues()
					
					if self.param_sync:
						self.param_sync.sync_on_batch_start()
					
					# æ‰§è¡Œæ‰¹æ¬¡è®­ç»ƒï¼ˆæµæ°´çº¿å¤„ç†ï¼‰
					loss = self._process_batch_with_reliability(batch, batch_idx)
					
					if loss is not None:
						# æ¢¯åº¦å¯é æ€§å¤„ç†
						if self.gradient_manager:
							if not self.gradient_manager.compute_gradients_with_backoff(loss, epoch, batch_idx):
								logger.warning(f"Batch {batch_idx} æ¢¯åº¦å¤„ç†å¤±è´¥ï¼Œè·³è¿‡")
								continue
						else:
							# æ ‡å‡†çš„åå‘ä¼ æ’­å’Œä¼˜åŒ–
							if self.rank == 6:  # åªæœ‰æœ€åä¸€ä¸ªrankè®¡ç®—loss
								if self.scaler:
									self.scaler.scale(loss).backward()
									self.scaler.step(self.optimizer)
									self.scaler.update()
								else:
									loss.backward()
									torch.nn.utils.clip_grad_norm_(self.all_params, max_norm=1.0)
									self.optimizer.step()
								
								self.optimizer.zero_grad()
						
						# æ‰¹æ¬¡ç»“æŸåçš„åŒæ­¥
						if self.param_sync:
							self.param_sync.sync_on_batch_end()
						
						# è®°å½•æ‰¹æ¬¡æŒ‡æ ‡åˆ°å¥åº·ç›‘æ§
						if self.health_monitor:
							self.health_monitor.record_batch_metrics(
								loss=loss.item() if loss is not None else 0.0,
								model=self.stage,
								optimizer=self.optimizer,
								epoch=epoch,
								batch_idx=batch_idx
							)
						
						# ç»Ÿè®¡
						if self.rank == 6:
							total_loss += loss.item()
							batch_count += 1
					
					# âœ… æ›´æ–°Batchè¿›åº¦æ¡
					batch_time = time.time() - batch_start_time
					
					if self.rank == 0:
						# Rank 0 æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
						batch_pbar.set_postfix({
							'Loss': f'{loss.item():.4f}' if loss else 'N/A',
							'Avg': f'{total_loss / max(batch_count, 1):.4f}',
							'Time': f'{batch_time:.2f}s',
							'Mem': f'{torch.cuda.memory_allocated() / 1e9:.1f}GB'
						})
					else:
						# å…¶ä»–rankæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
						batch_pbar.set_postfix({
							'Stage': self.stage_config['stage'][:8],
							'Time': f'{batch_time:.2f}s'
						})
					
					# ä¿æŒåŸæœ‰çš„å®šæœŸæ—¥å¿—ï¼ˆå‡å°‘é¢‘ç‡é¿å…åˆ·å±ï¼‰
					if batch_idx % (self.args.log_interval * 5) == 0 and self.rank == 0:
						batch_pbar.write(
							f"ğŸ“Š Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item() if loss else 'N/A':.6f}")
					
					self.training_state.batch = batch_idx
					self.training_state.global_step += 1
					
					# æ£€æŸ¥æ˜¯å¦åˆ°è¾¾epochç»“æŸï¼ˆé€šè¿‡ç‰¹æ®Šä¿¡å·ï¼‰
					if batch_idx > 0 and batch_idx % 100 == 0:
						# æ£€æŸ¥æ˜¯å¦æ”¶åˆ°ç»“æŸä¿¡å·
						if self._check_epoch_end_signal():
							break
				
				except Exception as e:
					# âœ… è¿›åº¦æ¡ä¸­æ˜¾ç¤ºé”™è¯¯
					batch_pbar.write(f"âŒ Rank {self.rank} Batch {batch_idx} é”™è¯¯: {e}")
					self._handle_training_error(e, batch_idx)
					continue
		
		finally:
			# âœ… ç¡®ä¿å…³é—­è¿›åº¦æ¡
			if hasattr(batch_pbar, 'close'):
				batch_pbar.close()
		
		# è®¡ç®—epochæŒ‡æ ‡
		epoch_time = time.time() - epoch_start_time
		self.training_state.training_time += epoch_time
		
		if self.rank == 6 and batch_count > 0:
			avg_loss = total_loss / batch_count
		else:
			avg_loss = 0.0
		
		return {
			'loss': avg_loss,
			'epoch_time': epoch_time,
			'batch_count': batch_count
		}
	
	
	
	
	def _process_batch_with_reliability(self, batch, batch_idx: int):
		"""å¸¦å¯é æ€§ä¿éšœçš„æ‰¹æ¬¡å¤„ç†"""
		try:
			if self.rank == 7:
				# DummyStageä¸å‚ä¸ä»»ä½•å®é™…å¤„ç†
				self.stage.process()  # è°ƒç”¨DummyStageçš„processæ–¹æ³•
				return None
			
			
			
			if self.rank == 0:
				# Rank 0: æ•°æ®é¢„å¤„ç†
				if batch is None:
					return None
				
				# æ‰§è¡Œé¢„å¤„ç†
				processed_data = self.stage.process(batch)
				
				# å‘é€åˆ°ä¸‹ä¸€ä¸ªstage
				for next_rank in self.next_ranks:
					self.node_comm.send_tensor(processed_data, next_rank)
				
				return None
			
			elif self.rank == 6:
				# Rank 6: åˆ†å‰²å¤´å’ŒæŸå¤±è®¡ç®—
				# ä»rank 5æ¥æ”¶æ•°æ®
				final_features = self.node_comm.recv_tensor(
					src_rank=self.prev_ranks[0],
					device=self.device
				)
				
				if final_features is None:
					return None
				
				# æ‰§è¡Œåˆ†å‰²å¤´
				predictions = self.stage.process(final_features)
				
				# è®¡ç®—æŸå¤±ï¼ˆéœ€è¦è·å–labelsï¼‰
				if self.loss_fn:
					# ä»rank 0è·å–å¯¹åº”çš„labels
					labels = self._receive_labels_from_rank0(batch_idx)
					if labels is not None:
						loss = self.loss_fn(predictions, labels)
						return loss
				
				return None
			
			else:
				# ä¸­é—´ranks: æ¥æ”¶å¤„ç†å‘é€
				# ä»å‰ä¸€ä¸ªrankæ¥æ”¶æ•°æ®
				input_data = None
				for prev_rank in self.prev_ranks:
					try:
						data = self.node_comm.recv_tensor(
							src_rank=prev_rank,
							device=self.device
						)
						if input_data is None:
							input_data = data
						else:
							# å¯¹äºrank 4ï¼Œéœ€è¦èåˆæ¥è‡ªrank 2å’Œ3çš„ç‰¹å¾
							input_data = self._fuse_features(input_data, data)
					except Exception as e:
						logger.warning(f"Rank {self.rank} æ¥æ”¶æ•°æ®å¤±è´¥: {e}")
						continue
				
				if input_data is None:
					return None
				
				# å¤„ç†æ•°æ®
				output_data = self.stage.process(input_data)
				
				# å‘é€åˆ°ä¸‹ä¸€ä¸ªrank
				for next_rank in self.next_ranks:
					self.node_comm.send_tensor(output_data, next_rank)
				
				return None
		
		except Exception as e:
			logger.error(f"Rank {self.rank} æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
			return None
	
	def _fuse_features(self, feature1, feature2):
		"""èåˆæ¥è‡ªä¸åŒåˆ†æ”¯çš„ç‰¹å¾"""
		if feature1 is None:
			return feature2
		if feature2 is None:
			return feature1
		
		# ç®€å•çš„ç‰¹å¾èåˆï¼šç›¸åŠ 
		try:
			if feature1.shape == feature2.shape:
				return feature1 + feature2
			else:
				# å¦‚æœå½¢çŠ¶ä¸åŒ¹é…ï¼Œå°è¯•è°ƒæ•´
				min_dim = min(feature1.shape[-1], feature2.shape[-1])
				return feature1[..., :min_dim] + feature2[..., :min_dim]
		except Exception as e:
			logger.warning(f"ç‰¹å¾èåˆå¤±è´¥: {e}")
			return feature1
	
	def _receive_labels_from_rank0(self, batch_idx: int):
		"""ä»rank 0æ¥æ”¶å¯¹åº”batchçš„labels"""
		try:
			# é€šè¿‡ç‰¹æ®Šçš„é€šä¿¡tagæ¥æ”¶labels
			labels = self.node_comm.recv_tensor(
				src_rank=0,
				tag=1000 + batch_idx,
				device=self.device
			)
			return labels
		except Exception as e:
			logger.warning(f"æ¥æ”¶labelså¤±è´¥: {e}")
			return None
	
	def _check_epoch_end_signal(self) -> bool:
		"""æ£€æŸ¥æ˜¯å¦æ”¶åˆ°epochç»“æŸä¿¡å·"""
		try:
			# æ£€æŸ¥æ˜¯å¦æœ‰ç»“æŸä¿¡å·
			if self.rank != 0:
				signal_tensor = torch.zeros(1, dtype=torch.long, device=self.device)
				dist.recv(signal_tensor, src=0, tag=9999)
				return signal_tensor.item() == -1
			return False
		except:
			return False
	
	def _handle_health_issues(self):
		"""å¤„ç†å¥åº·é—®é¢˜"""
		logger.warning(f"Rank {self.rank} æ£€æµ‹åˆ°å¥åº·é—®é¢˜ï¼Œå°è¯•æ¢å¤...")
		
		# ç®€å•çš„æ¢å¤ç­–ç•¥ï¼šæ¸…ç†GPUå†…å­˜
		if torch.cuda.is_available():
			torch.cuda.empty_cache()
			torch.cuda.synchronize()
		
		time.sleep(1)  # çŸ­æš‚ç­‰å¾…
	
	def _handle_training_error(self, error: Exception, batch_idx: int):
		"""å¤„ç†è®­ç»ƒé”™è¯¯"""
		logger.error(f"Rank {self.rank} Batch {batch_idx} è®­ç»ƒé”™è¯¯: {error}")
		
		# æ ¹æ®é”™è¯¯ç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
		if "CUDA out of memory" in str(error):
			torch.cuda.empty_cache()
			logger.info("GPUå†…å­˜ä¸è¶³ï¼Œå·²æ¸…ç†ç¼“å­˜")
		elif "communication" in str(error).lower():
			logger.info("é€šä¿¡é”™è¯¯ï¼Œç­‰å¾…é‡è¯•")
			time.sleep(2)
		else:
			logger.info("æœªçŸ¥é”™è¯¯ï¼Œè·³è¿‡è¯¥batch")
	
	def _post_epoch_sync_and_save(self, epoch: int, epoch_metrics: Dict[str, float]):
		"""Epochç»“æŸåçš„åŒæ­¥å’Œä¿å­˜"""
		
		# å‚æ•°åŒæ­¥
		if self.param_sync:
			if not self.param_sync.sync_on_epoch_end():
				logger.warning(f"Epoch {epoch} ç»“æŸå‚æ•°åŒæ­¥å¤±è´¥")
		
		# å…¨å±€åŒæ­¥
		dist.barrier()
		
		# ä¿å­˜æ£€æŸ¥ç‚¹
		if epoch % self.args.save_interval == 0:
			self._save_distributed_checkpoint(epoch, epoch_metrics)
		
		# æ›´æ–°å­¦ä¹ ç‡
		self.scheduler.step()
		
		# æ›´æ–°æœ€ä½³æŒ‡æ ‡
		if self.rank == 6:
			current_loss = epoch_metrics.get('loss', float('inf'))
			if current_loss < self.training_state.best_loss:
				self.training_state.best_loss = current_loss
	
	def _save_distributed_checkpoint(self, epoch: int, epoch_metrics: Dict[str, float]):
		"""ä¿å­˜åˆ†å¸ƒå¼æ£€æŸ¥ç‚¹"""
		try:
			success = self.distributed_checkpoint.save_distributed_checkpoint(
				model=self.stage,
				optimizer=self.optimizer,
				scheduler=self.scheduler,
				epoch=epoch,
				batch_idx=self.training_state.batch,
				global_step=self.training_state.global_step,
				train_loss=epoch_metrics.get('loss', 0.0),
				val_loss=None,  # éªŒè¯lossåœ¨éªŒè¯æ—¶è®¾ç½®
				extra_data={
					'config': self.config,
					'args': vars(self.args),
					'training_state': self.training_state.__dict__
				}
			)
			
			if success and self.rank == 0:
				logger.info(f"Epoch {epoch} æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ")
		
		except Exception as e:
			logger.error(f"Rank {self.rank} æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
	
	def validate_epoch(self, epoch: int) -> float:
		"""
		éªŒè¯ä¸€ä¸ªepoch

		Args:
			epoch: å½“å‰epoch

		Returns:
			éªŒè¯æŸå¤±
		"""
		if self.rank != 6:  # åªæœ‰æœ€åä¸€ä¸ªrankåšéªŒè¯
			return 0.0
		
		if not hasattr(self, 'val_loader') or self.val_loader is None:
			return 0.0
		
		# è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
		self.stage.eval()
		
		try:
			# ä½¿ç”¨è¯„ä¼°ç®¡ç†å™¨è¿›è¡ŒéªŒè¯
			val_metrics = self.evaluator.evaluate(self, self.val_loader, epoch)
			
			if val_metrics:
				# è®°å½•éªŒè¯æŒ‡æ ‡åˆ°tensorboard
				self._log_validation_metrics(val_metrics, epoch)
				
				# æ›´æ–°æœ€ä½³diceåˆ†æ•°
				dice_score = val_metrics.get('dice_score', 0.0)
				if dice_score > self.training_state.best_dice:
					self.training_state.best_dice = dice_score
				
				return val_metrics.get('dice_score', 0.0)
		
		except Exception as e:
			logger.error(f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
		
		finally:
			# æ¢å¤è®­ç»ƒæ¨¡å¼
			self.stage.train()
		
		return 0.0
	
	def _log_validation_metrics(self, metrics: Dict[str, float], epoch: int):
		"""è®°å½•éªŒè¯æŒ‡æ ‡åˆ°tensorboard"""
		if self.logger is None:
			return
		
		try:
			# æ ¸å¿ƒåˆ†å‰²æŒ‡æ ‡
			self.logger.log_scalar('validation/dice_score', metrics.get('dice_score', 0), epoch)
			self.logger.log_scalar('validation/iou_score', metrics.get('iou_score', 0), epoch)
			self.logger.log_scalar('validation/precision', metrics.get('precision', 0), epoch)
			self.logger.log_scalar('validation/recall', metrics.get('recall', 0), epoch)
			
			# Hausdorffè·ç¦»
			if 'hausdorff_distance' in metrics:
				self.logger.log_scalar('validation/hausdorff_distance', metrics['hausdorff_distance'], epoch)
			
			# åˆ†ç±»åˆ«æŒ‡æ ‡
			if 'vessel_dice' in metrics:
				self.logger.log_scalar('validation/vessel_dice', metrics['vessel_dice'], epoch)
			if 'tumor_dice' in metrics:
				self.logger.log_scalar('validation/tumor_dice', metrics['tumor_dice'], epoch)
			
			# Tierçº§åˆ«æŒ‡æ ‡
			for tier in [0, 1, 2]:
				tier_key = f'tier_{tier}_dice'
				if tier_key in metrics:
					self.logger.log_scalar(f'validation/{tier_key}', metrics[tier_key], epoch)
			
			# è®­ç»ƒæŸå¤±
			if hasattr(self, '_last_train_loss'):
				self.logger.log_scalar('training/loss', self._last_train_loss, epoch)
			
			# å­¦ä¹ ç‡
			current_lr = self.scheduler.get_last_lr()[0]
			self.logger.log_scalar('training/learning_rate', current_lr, epoch)
			
			logger.info(f"éªŒè¯æŒ‡æ ‡å·²è®°å½•åˆ°tensorboard: Dice={metrics.get('dice_score', 0):.4f}")
		
		except Exception as e:
			logger.error(f"è®°å½•éªŒè¯æŒ‡æ ‡å¤±è´¥: {e}")
	
	def finalize_training(self):
		"""è®­ç»ƒç»“æŸåçš„æ¨¡å‹åˆå¹¶å’Œå¯¼å‡º"""
		
		if self.rank == 0:  # åªåœ¨ä¸»rankæ‰§è¡Œ
			logger.info("å¼€å§‹æœ€ç»ˆæ¨¡å‹åˆå¹¶å’Œå¯¼å‡º...")
			
			try:
				# æ”¶é›†æ‰€æœ‰rankçš„æœ€æ–°checkpoints
				checkpoint_dir = Path(self.args.output_dir) / 'checkpoints'
				
				# æ‰¾åˆ°æœ€æ–°çš„epoch
				latest_epoch = self._find_latest_epoch(checkpoint_dir)
				if latest_epoch is None:
					logger.error("æœªæ‰¾åˆ°æœ‰æ•ˆçš„checkpointæ–‡ä»¶")
					return
				
				all_checkpoints = self.model_merger.collect_all_checkpoints(
					str(checkpoint_dir),
					latest_epoch
				)
				
				if len(all_checkpoints) < self.world_size:
					logger.warning(f"åªæ”¶é›†åˆ° {len(all_checkpoints)}/{self.world_size} ä¸ªcheckpoints")
				
				# åˆå¹¶ä¸ºå®Œæ•´æ¨¡å‹
				merged_model = self.model_merger.merge_pipeline_checkpoints(all_checkpoints)
				
				# å¯¼å‡ºä¸ºéƒ¨ç½²ç”¨æ¨¡å‹
				export_config = self.config.get('model_export', {})
				export_dir = Path(export_config.get('export_dir', './exported_models'))
				export_dir.mkdir(parents=True, exist_ok=True)
				
				export_path = export_dir / f'deployed_model_epoch_{latest_epoch}'
				success = self.model_merger.export_for_deployment(merged_model, str(export_path))
				
				if success:
					logger.info(f"æ¨¡å‹æˆåŠŸå¯¼å‡ºåˆ°: {export_path}")
					
					# è®°å½•æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
					final_stats = {
						'total_epochs': latest_epoch,
						'total_training_time': self.training_state.training_time,
						'best_dice_score': self.training_state.best_dice,
						'best_loss': self.training_state.best_loss,
						'export_path': str(export_path)
					}
					
					logger.info(f"è®­ç»ƒå®Œæˆç»Ÿè®¡: {final_stats}")
					
					# ä¿å­˜è®­ç»ƒæ‘˜è¦
					summary_path = Path(self.args.output_dir) / 'training_summary.yaml'
					with open(summary_path, 'w') as f:
						yaml.dump(final_stats, f, default_flow_style=False)
			
			except Exception as e:
				logger.error(f"æ¨¡å‹åˆå¹¶å’Œå¯¼å‡ºå¤±è´¥: {e}")
				traceback.print_exc()
		
		# åœæ­¢ç›‘æ§
		if self.health_monitor:
			self.health_monitor.stop_monitoring()
		
		# å…¨å±€åŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹å®Œæˆ
		dist.barrier()
		
		if self.rank == 0:
			logger.info("åˆ†å¸ƒå¼è®­ç»ƒå®Œå…¨ç»“æŸ")
	
	def _find_latest_epoch(self, checkpoint_dir: Path) -> Optional[int]:
		"""æ‰¾åˆ°æœ€æ–°çš„epoch"""
		if not checkpoint_dir.exists():
			return None
		
		latest_epoch = None
		for checkpoint_file in checkpoint_dir.glob("checkpoint_epoch_*_rank_0.pt"):
			try:
				# ä»æ–‡ä»¶åæå–epoch
				parts = checkpoint_file.stem.split('_')
				epoch_idx = parts.index('epoch') + 1
				epoch = int(parts[epoch_idx])
				
				if latest_epoch is None or epoch > latest_epoch:
					latest_epoch = epoch
			except (ValueError, IndexError):
				continue
		
		return latest_epoch


def setup_distributed():
	"""è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ"""
	if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
		rank = int(os.environ['RANK'])
		world_size = int(os.environ['WORLD_SIZE'])
		local_rank = int(os.environ['LOCAL_RANK'])
	else:
		raise RuntimeError("åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡æœªæ­£ç¡®è®¾ç½®")
	
	# åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
	dist.init_process_group(
		backend='nccl',
		init_method='env://',
		world_size=world_size,
		rank=rank
	)
	
	# è®¾ç½®å½“å‰è®¾å¤‡
	torch.cuda.set_device(local_rank)
	
	return rank, world_size, local_rank


def parse_args():
	"""è§£æå‘½ä»¤è¡Œå‚æ•°"""
	parser = argparse.ArgumentParser(description='é›†æˆåŒ–æµæ°´çº¿åˆ†å¸ƒå¼è®­ç»ƒ')
	
	# æ•°æ®å‚æ•°
	parser.add_argument('--image_dir', type=str, required=True, help='å›¾åƒç›®å½•è·¯å¾„')
	parser.add_argument('--label_dir', type=str, required=True, help='æ ‡ç­¾ç›®å½•è·¯å¾„')
	parser.add_argument('--output_dir', type=str, default='./output', help='è¾“å‡ºç›®å½•è·¯å¾„')
	parser.add_argument('--config', type=str, default='configs/default.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
	
	# è®­ç»ƒå‚æ•°
	parser.add_argument('--batch_size', type=int, default=1, help='æ‰¹æ¬¡å¤§å°')
	parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
	parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
	parser.add_argument('--num_workers', type=int, default=4, help='æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°')
	parser.add_argument('--val_interval', type=int, default=5, help='éªŒè¯é—´éš”')
	parser.add_argument('--save_interval', type=int, default=10, help='ä¿å­˜é—´éš”')
	parser.add_argument('--log_interval', type=int, default=10, help='æ—¥å¿—é—´éš”')
	
	# å…¶ä»–å‚æ•°
	parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
	parser.add_argument('--amp', action='store_true', help='ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦')
	
	return parser.parse_args()


def load_config(config_path: str) -> Dict:
	"""åŠ è½½é…ç½®æ–‡ä»¶"""
	config_path = Path(config_path)
	
	if not config_path.exists():
		logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
		return {}
	
	try:
		with open(config_path, 'r', encoding='utf-8') as f:
			config = yaml.safe_load(f)
		
		logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
		return config
	
	except Exception as e:
		logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
		return {}


def cleanup_on_exit(trainer):
	"""é€€å‡ºæ—¶çš„æ¸…ç†å‡½æ•°"""
	
	def signal_handler(signum, frame):
		logger.info(f"Rank {trainer.rank}: æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œå¼€å§‹æ¸…ç†...")
		
		try:
			# ä¿å­˜ç´§æ€¥æ£€æŸ¥ç‚¹
			trainer._save_distributed_checkpoint(
				trainer.training_state.epoch,
				{'loss': trainer.training_state.best_loss}
			)
			
			# åœæ­¢ç›‘æ§
			if trainer.health_monitor:
				trainer.health_monitor.stop_monitoring()
			
			# æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
			if dist.is_initialized():
				dist.destroy_process_group()
		
		except Exception as e:
			logger.error(f"æ¸…ç†è¿‡ç¨‹å‡ºé”™: {e}")
		
		sys.exit(0)
	
	signal.signal(signal.SIGINT, signal_handler)
	signal.signal(signal.SIGTERM, signal_handler)


def main():
	"""ä¸»å‡½æ•°"""
	try:
		# è§£æå‚æ•°å’Œè®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
		args = parse_args()
		rank, world_size, local_rank = setup_distributed()
		
		# åŠ è½½é…ç½®
		config = load_config(args.config)
		
		# åˆ›å»ºè¾“å‡ºç›®å½•
		if rank == 0:
			Path(args.output_dir).mkdir(parents=True, exist_ok=True)
			logger.info(f"è¾“å‡ºç›®å½•åˆ›å»º: {args.output_dir}")
		
		logger.info(f"Rank {rank}: å‡†å¤‡æ‰§è¡ŒbarrieråŒæ­¥...")
		
		# è®©rankæŒ‰é¡ºåºåˆå§‹åŒ–ï¼Œé¿å…åŒæ—¶ç«äº‰èµ„æº
		time.sleep(rank * 2)  # æ¯ä¸ªrankå»¶è¿Ÿä¸åŒæ—¶é—´
		
		
		# å…¨å±€åŒæ­¥
		dist.barrier()
		
		logger.info(f"Rank {rank}: å¼€å§‹é›†æˆåŒ–æµæ°´çº¿è®­ç»ƒ")
		
		# åˆ›å»ºé›†æˆåŒ–è®­ç»ƒå™¨
		trainer = IntegratedPipelineTrainer(rank, world_size, local_rank, config, args)
		
		# è®¾ç½®é€€å‡ºæ¸…ç†
		cleanup_on_exit(trainer)
		
		# âœ… æ·»åŠ Epochçº§åˆ«è¿›åº¦æ¡
		best_val_score = 0.0
		
		if rank == 0:
			# åªæœ‰ä¸»è¿›ç¨‹æ˜¾ç¤ºæ€»ä½“è¿›åº¦æ¡
			epoch_pbar = tqdm(
				range(args.epochs),
				desc="ğŸš€ Training Progress",
				ncols=100,
				colour='green'
			)
		else:
			epoch_pbar = range(args.epochs)
		
		for epoch in epoch_pbar:
			try:
				# è®­ç»ƒä¸€ä¸ªepoch
				epoch_start_time = time.time()
				epoch_metrics = trainer.train_epoch(epoch)
				epoch_time = time.time() - epoch_start_time
				
				# è®°å½•è®­ç»ƒlossï¼ˆç”¨äºtensorboardï¼‰
				if rank == 6:
					trainer._last_train_loss = epoch_metrics.get('loss', 0.0)
				
				# å…¨å±€åŒæ­¥epochå®Œæˆ
				dist.barrier()
				
				# éªŒè¯ï¼ˆæ¯å‡ ä¸ªepochä¸€æ¬¡ï¼‰
				val_score = 0.0
				if epoch % args.val_interval == 0:
					val_score = trainer.validate_epoch(epoch)
					
					# æ›´æ–°æœ€ä½³éªŒè¯åˆ†æ•°
					if val_score > best_val_score:
						best_val_score = val_score
						
						# ä¿å­˜æœ€ä½³æ¨¡å‹
						if rank == 0:
							logger.info(f"æ–°çš„æœ€ä½³éªŒè¯åˆ†æ•°: {best_val_score:.4f}")
				
				# âœ… æ›´æ–°Epochè¿›åº¦æ¡
				if rank == 0:
					epoch_pbar.set_postfix({
						'Loss': f'{epoch_metrics.get("loss", 0.0):.4f}',
						'Val': f'{val_score:.4f}',
						'Best': f'{best_val_score:.4f}',
						'Time': f'{epoch_time:.1f}s',
						'GPU': f'{torch.cuda.memory_allocated() / 1e9:.1f}GB'
					})
					
					# åŒæ—¶ä¿æŒåŸæœ‰çš„è¯¦ç»†æ—¥å¿—
					if epoch % 5 == 0:  # æ¯5ä¸ªepochè¯¦ç»†æ—¥å¿—ä¸€æ¬¡
						logger.info(
							f'Epoch {epoch}: '
							f'Train Loss: {epoch_metrics.get("loss", 0.0):.6f}, '
							f'Val Score: {val_score:.4f}, '
							f'Time: {epoch_time:.2f}s'
						)
				
				# å†…å­˜æ¸…ç†
				if epoch % 10 == 0:
					torch.cuda.empty_cache()
			
			except Exception as e:
				if rank == 0:
					epoch_pbar.write(f"âŒ Epoch {epoch} è®­ç»ƒå¤±è´¥: {e}")
				logger.error(f"Epoch {epoch} è®­ç»ƒå¤±è´¥: {e}")
				traceback.print_exc()
				
				# å°è¯•æ¢å¤è®­ç»ƒ
				if "CUDA out of memory" in str(e):
					torch.cuda.empty_cache()
					if rank == 0:
						epoch_pbar.write("ğŸ§¹ GPUå†…å­˜æ¸…ç†å®Œæˆï¼Œå°è¯•ç»§ç»­è®­ç»ƒ")
					continue
				else:
					raise e
		
		# âœ… å…³é—­è¿›åº¦æ¡
		if rank == 0:
			epoch_pbar.close()
			print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
		
		# è®­ç»ƒç»“æŸåçš„æ¨¡å‹åˆå¹¶
		trainer.finalize_training()
		
		# æœ€ç»ˆåŒæ­¥
		dist.barrier()
		
		if rank == 0:
			logger.info("ğŸ‰ é›†æˆåŒ–åˆ†å¸ƒå¼è®­ç»ƒæˆåŠŸå®Œæˆ!")
			logger.info(f"æœ€ä½³éªŒè¯åˆ†æ•°: {best_val_score:.4f}")
			logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {trainer.training_state.training_time:.2f}ç§’")
	
	except Exception as e:
		logger.error(f"Rank {rank}: è®­ç»ƒå¤±è´¥: {e}")
		traceback.print_exc()
		raise
	
	finally:
		# æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
		try:
			if dist.is_initialized():
				dist.destroy_process_group()
		except Exception as e:
			logger.debug(f"æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒå¤±è´¥: {e}")


if __name__ == '__main__':
	main()