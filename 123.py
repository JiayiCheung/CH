#123.py
"""
åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯NCCLé€šä¿¡æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import torch
import torch.distributed as dist
import time
import logging


def setup_logging():
	logging.basicConfig(
		level=logging.INFO,
		format='[Rank %(rank)s] %(asctime)s - %(levelname)s - %(message)s',
		handlers=[
			logging.StreamHandler(),
			logging.FileHandler(f'dist_test_rank_{os.environ.get("RANK", "unknown")}.log')
		]
	)


def test_distributed():
	"""æµ‹è¯•åˆ†å¸ƒå¼ç¯å¢ƒ"""
	
	# è·å–ç¯å¢ƒå˜é‡
	rank = int(os.environ['RANK'])
	world_size = int(os.environ['WORLD_SIZE'])
	local_rank = int(os.environ['LOCAL_RANK'])
	master_addr = os.environ['MASTER_ADDR']
	master_port = os.environ['MASTER_PORT']
	
	logger = logging.getLogger()
	logger.addFilter(lambda record: setattr(record, 'rank', rank) or True)
	
	logger.info(f"å¼€å§‹åˆ†å¸ƒå¼æµ‹è¯•")
	logger.info(f"Rank: {rank}/{world_size}, Local Rank: {local_rank}")
	logger.info(f"Master: {master_addr}:{master_port}")
	
	# è®¾ç½®CUDAè®¾å¤‡
	torch.cuda.set_device(local_rank)
	device = torch.device(f'cuda:{local_rank}')
	logger.info(f"CUDAè®¾å¤‡: {device}")
	
	try:
		# åˆå§‹åŒ–è¿›ç¨‹ç»„
		logger.info("åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„...")
		dist.init_process_group(
			backend='nccl',
			init_method='env://',
			world_size=world_size,
			rank=rank,
			timeout=torch.distributed.default_pg_timeout  # ä½¿ç”¨é»˜è®¤è¶…æ—¶
		)
		logger.info("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„åˆå§‹åŒ–æˆåŠŸ")
		
		# æµ‹è¯•åŸºæœ¬é€šä¿¡
		logger.info("æµ‹è¯•åŸºæœ¬é€šä¿¡...")
		tensor = torch.ones(2, device=device) * rank
		dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
		expected = sum(range(world_size)) * torch.ones(2, device=device)
		
		if torch.allclose(tensor, expected):
			logger.info("âœ… All-reduceé€šä¿¡æµ‹è¯•æˆåŠŸ")
		else:
			logger.error(f"âŒ All-reduceæµ‹è¯•å¤±è´¥: æœŸæœ› {expected}, å¾—åˆ° {tensor}")
		
		# æµ‹è¯•å¹¿æ’­
		if rank == 0:
			broadcast_tensor = torch.randn(10, device=device)
		else:
			broadcast_tensor = torch.zeros(10, device=device)
		
		dist.broadcast(broadcast_tensor, src=0)
		logger.info("âœ… å¹¿æ’­é€šä¿¡æµ‹è¯•æˆåŠŸ")
		
		# åŒæ­¥æµ‹è¯•
		logger.info("æµ‹è¯•åŒæ­¥å±éšœ...")
		dist.barrier()
		logger.info("âœ… åŒæ­¥å±éšœæµ‹è¯•æˆåŠŸ")
		
		# å»¶è¿Ÿæµ‹è¯•
		start_time = time.time()
		for i in range(10):
			test_tensor = torch.randn(1000, device=device)
			dist.all_reduce(test_tensor)
		elapsed = time.time() - start_time
		logger.info(f"âœ… é€šä¿¡å»¶è¿Ÿæµ‹è¯•å®Œæˆ: {elapsed:.3f}s for 10 iterations")
		
		logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åˆ†å¸ƒå¼ç¯å¢ƒæ­£å¸¸")
	
	except Exception as e:
		logger.error(f"âŒ åˆ†å¸ƒå¼æµ‹è¯•å¤±è´¥: {e}")
		import traceback
		traceback.print_exc()
		return False
	
	finally:
		if dist.is_initialized():
			dist.destroy_process_group()
	
	return True


if __name__ == "__main__":
	setup_logging()
	
	# æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
	required_env = ['RANK', 'WORLD_SIZE', 'LOCAL_RANK', 'MASTER_ADDR', 'MASTER_PORT']
	missing = [var for var in required_env if var not in os.environ]
	
	if missing:
		print(f"âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: {missing}")
		exit(1)
	
	success = test_distributed()
	exit(0 if success else 1)